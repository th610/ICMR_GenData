"""
Judge í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ë¡œê·¸ë¡œ ì €ì¥

judge_results_summary_window.jsonì„ ì½ì–´ì„œ:
1. í†µê³„ ìš”ì•½
2. í´ë˜ìŠ¤ë³„ ì •í™•ë„
3. ì˜¤íŒ íŒ¨í„´ ë¶„ì„
4. ì‹¤íŒ¨ ì‚¬ë¡€ ìƒì„¸
â†’ EVALUATION_LOG.mdë¡œ ì €ì¥
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter


def analyze_judge_results(results_file: Path):
    """Judge ê²°ê³¼ ë¶„ì„"""
    
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    timestamp = data.get('timestamp', 'Unknown')
    results = data.get('results', {})
    
    # í†µê³„ ìˆ˜ì§‘
    stats = {
        'total': 0,
        'match': 0,
        'mismatch': 0,
        'by_class': {},
        'confusion_matrix': Counter(),
        'mismatches': []
    }
    
    for class_name, sessions in results.items():
        class_total = len(sessions)
        class_match = sum(1 for s in sessions if s.get('matches'))
        class_mismatch = class_total - class_match
        
        stats['total'] += class_total
        stats['match'] += class_match
        stats['mismatch'] += class_mismatch
        
        stats['by_class'][class_name] = {
            'total': class_total,
            'match': class_match,
            'mismatch': class_mismatch,
            'accuracy': (class_match / class_total * 100) if class_total > 0 else 0
        }
        
        # Confusion matrix
        for session in sessions:
            expected = session.get('expected_label', 'Unknown')
            predicted = session.get('predicted_label', 'Unknown')
            if not session.get('matches'):
                stats['confusion_matrix'][(expected, predicted)] += 1
                stats['mismatches'].append({
                    'session_id': session.get('session_id'),
                    'expected': expected,
                    'predicted': predicted,
                    'reason': session.get('reason', ''),
                    'confidence': session.get('confidence', '')
                })
    
    return stats, timestamp


def generate_log_markdown(stats: dict, timestamp: str, output_file: Path):
    """Markdown ë¡œê·¸ ìƒì„±"""
    
    lines = []
    lines.append("# Judge í‰ê°€ ê²°ê³¼ ë¡œê·¸")
    lines.append(f"\n**í‰ê°€ ì¼ì‹œ**: {timestamp}")
    lines.append(f"\n---\n")
    
    # ì „ì²´ í†µê³„
    lines.append("## ğŸ“Š ì „ì²´ í†µê³„\n")
    accuracy = (stats['match'] / stats['total'] * 100) if stats['total'] > 0 else 0
    lines.append(f"- **ì „ì²´ ì„¸ì…˜**: {stats['total']}ê°œ")
    lines.append(f"- **ì •í™•íˆ íŒì •**: {stats['match']}ê°œ")
    lines.append(f"- **ì˜¤íŒ**: {stats['mismatch']}ê°œ")
    lines.append(f"- **ì „ì²´ ì •í™•ë„**: {accuracy:.1f}%\n")
    
    # í´ë˜ìŠ¤ë³„ ì •í™•ë„
    lines.append("## ğŸ“ˆ í´ë˜ìŠ¤ë³„ ì •í™•ë„\n")
    lines.append("| í´ë˜ìŠ¤ | ì „ì²´ | ì •í™• | ì˜¤íŒ | ì •í™•ë„ |")
    lines.append("|--------|------|------|------|--------|")
    
    for class_name in ['normal', 'v1', 'v2', 'v3', 'v4', 'v5']:
        if class_name in stats['by_class']:
            cls_stats = stats['by_class'][class_name]
            lines.append(
                f"| {class_name.upper():6s} | "
                f"{cls_stats['total']:4d} | "
                f"{cls_stats['match']:4d} | "
                f"{cls_stats['mismatch']:4d} | "
                f"{cls_stats['accuracy']:6.1f}% |"
            )
    
    lines.append("")
    
    # Confusion Matrix
    lines.append("## ğŸ”„ ì˜¤íŒ íŒ¨í„´ (Confusion Matrix)\n")
    if stats['confusion_matrix']:
        lines.append("| ì‹¤ì œ ë¼ë²¨ | Judge íŒì • | íšŸìˆ˜ |")
        lines.append("|-----------|------------|------|")
        for (expected, predicted), count in stats['confusion_matrix'].most_common():
            lines.append(f"| {expected:9s} | {predicted:10s} | {count:4d} |")
        lines.append("")
    else:
        lines.append("*(ì˜¤íŒ ì—†ìŒ)*\n")
    
    # ì‹¤íŒ¨ ì‚¬ë¡€ ìƒì„¸
    lines.append("## âŒ ì‹¤íŒ¨ ì‚¬ë¡€ ìƒì„¸\n")
    
    if stats['mismatches']:
        # í´ë˜ìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
        by_class = {}
        for m in stats['mismatches']:
            expected = m['expected']
            if expected not in by_class:
                by_class[expected] = []
            by_class[expected].append(m)
        
        for class_name in ['Normal', 'V1', 'V2', 'V3', 'V4', 'V5']:
            if class_name in by_class:
                lines.append(f"### {class_name}\n")
                for m in by_class[class_name]:
                    lines.append(f"**[{m['session_id']}]** {m['expected']} â†’ {m['predicted']} ({m['confidence']})")
                    lines.append(f"- ì´ìœ : {m['reason']}\n")
    else:
        lines.append("*(ì‹¤íŒ¨ ì‚¬ë¡€ ì—†ìŒ)*\n")
    
    # ê²°ë¡  ë° ì œì•ˆ
    lines.append("---\n")
    lines.append("## ğŸ’¡ ë¶„ì„ ë° ì œì•ˆ\n")
    
    # ì„±ê³µí•œ í´ë˜ìŠ¤
    success_classes = [
        cls for cls, data in stats['by_class'].items()
        if data['accuracy'] >= 80
    ]
    
    # ì‹¤íŒ¨í•œ í´ë˜ìŠ¤
    fail_classes = [
        cls for cls, data in stats['by_class'].items()
        if data['accuracy'] < 50
    ]
    
    if success_classes:
        lines.append(f"**âœ… ì„±ê³µ í´ë˜ìŠ¤** (ì •í™•ë„ â‰¥80%): {', '.join([c.upper() for c in success_classes])}\n")
    
    if fail_classes:
        lines.append(f"**âŒ ê°œì„  í•„ìš”** (ì •í™•ë„ <50%): {', '.join([c.upper() for c in fail_classes])}\n")
    
    lines.append("\n**ë‹¤ìŒ ë‹¨ê³„:**")
    
    if accuracy >= 70:
        lines.append("- âœ… ì „ì²´ ì •í™•ë„ ì–‘í˜¸ - ì „ì²´ ë°ì´í„° ìƒì„± ì§„í–‰ ê°€ëŠ¥")
    elif accuracy >= 50:
        lines.append("- âš ï¸ ì‹¤íŒ¨ í´ë˜ìŠ¤ ìƒì„± í”„ë¡¬í”„íŠ¸ ê°œì„  í•„ìš”")
        lines.append("- ê°œì„  í›„ ì¬ìƒì„± ë° ì¬í‰ê°€ ê¶Œì¥")
    else:
        lines.append("- âŒ ìƒì„± í”„ë¡¬í”„íŠ¸ ì „ë©´ ì¬ê²€í†  í•„ìš”")
        lines.append("- Judge í”„ë¡¬í”„íŠ¸ì™€ ìƒì„± í”„ë¡¬í”„íŠ¸ ì •í•©ì„± í™•ì¸")
    
    # íŒŒì¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    return '\n'.join(lines)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    results_file = Path("data/pilot/judge_results_summary_window.json")
    output_file = Path("EVALUATION_LOG.md")
    
    if not results_file.exists():
        print(f"âŒ Judge ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {results_file}")
        return
    
    print("=" * 80)
    print("Judge í‰ê°€ ê²°ê³¼ ë¶„ì„ ë° ë¡œê·¸ ìƒì„±")
    print("=" * 80)
    print()
    
    # ë¶„ì„
    print("ğŸ“Š ê²°ê³¼ ë¶„ì„ ì¤‘...")
    stats, timestamp = analyze_judge_results(results_file)
    
    # ë¡œê·¸ ìƒì„±
    print("ğŸ“ ë¡œê·¸ ìƒì„± ì¤‘...")
    log_content = generate_log_markdown(stats, timestamp, output_file)
    
    print(f"\nâœ… ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {output_file}")
    print()
    
    # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
    print("=" * 80)
    print("ğŸ“Š ìš”ì•½")
    print("=" * 80)
    accuracy = (stats['match'] / stats['total'] * 100) if stats['total'] > 0 else 0
    print(f"ì „ì²´ ì •í™•ë„: {accuracy:.1f}% ({stats['match']}/{stats['total']})")
    print()
    
    print("í´ë˜ìŠ¤ë³„:")
    for class_name in ['normal', 'v1', 'v2', 'v3', 'v4', 'v5']:
        if class_name in stats['by_class']:
            cls_stats = stats['by_class'][class_name]
            status = "âœ…" if cls_stats['accuracy'] >= 80 else "âš ï¸" if cls_stats['accuracy'] >= 50 else "âŒ"
            print(f"  {status} {class_name.upper():6s}: {cls_stats['accuracy']:5.1f}% ({cls_stats['match']}/{cls_stats['total']})")
    
    print()
    print("=" * 80)


if __name__ == "__main__":
    main()
