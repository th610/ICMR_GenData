# ë°ì´í„° ìƒì„± ë° í•™ìŠµ ì „ì²´ ì ˆì°¨ (ìƒì„¸ ê°€ì´ë“œ)

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ

**ESConv Violation Detector PoC**
- ì •ì„œ ì§€ì› ìƒë‹´(Emotional Support Conversation)ì—ì„œ ìƒë‹´ì‚¬(supporter)ì˜ ì‘ë‹µ í’ˆì§ˆì„ ìë™ í‰ê°€
- 5ê°€ì§€ ìœ„ë°˜ ìœ í˜•(V1~V5)ì„ multi-label classificationìœ¼ë¡œ íƒì§€
- í•©ì„± ë°ì´í„° ìƒì„± + LLM-judge ë¼ë²¨ë§ + Transformer ê¸°ë°˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

**ìœ„ë°˜ ìœ í˜• ì •ì˜:**
- **V1 (Missing Context & Info-Gathering)**: ë‚´ë‹´ìì˜ ë§¥ë½ì„ ì¶©ë¶„íˆ íŒŒì•…í•˜ì§€ ì•Šê³  ì‘ë‹µ. í•„ìš”í•œ ì •ë³´ë¥¼ ë¬»ì§€ ì•ŠìŒ.
- **V2 (Agency Violation)**: ë‚´ë‹´ìì˜ ììœ¨ì„±ê³¼ ì£¼ë„ê¶Œì„ ì¹¨í•´. ì§€ì‹œì /ê°•ì••ì  ì¡°ì–¸.
- **V3 (Low-Quality Empathy)**: ì§„ë¶€í•˜ê³  í˜•ì‹ì ì¸ ê³µê° í‘œí˜„. "í˜ë‚´ì„¸ìš”", "ì´í•´í•´ìš”" ê°™ì€ í”¼ìƒì  ì‘ë‹µ.
- **V4 (Reality Distortion)**: ë‚´ë‹´ìì˜ í˜„ì‹¤ì´ë‚˜ ê°ì •ì„ ì™œê³¡í•˜ê±°ë‚˜ ë¬´ì‹œ. ê³¼ë„í•œ ê¸ì •ì£¼ì˜.
- **V5 (Crisis Safety Failure)**: ìœ„ê¸° ìƒí™©(ìì‚´, ìí•´ ë“±)ì—ì„œ ì•ˆì „ í”„ë¡œí† ì½œ ë¬´ì‹œ.

---

## ğŸ“‹ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê°œìš”

```
ESConv.json (1300 ì„¸ì…˜, Liu et al. 2021)
    â†“
[Step 1] ì›ë³¸ ìƒ˜í”Œë§
    â†“
50 ì›ë³¸ ì„¸ì…˜ (1361 turns)
    â†“
[Step 2] í•©ì„± ìƒì„± (ìœ„ë°˜ ì£¼ì…)
    â†“  LLM(gpt-4o-mini) single-turn rewrite
50 í•©ì„± ì„¸ì…˜ (ê° 1ê°œ ìœ„ë°˜)
    â†“
[Step 3] Train/Val/Test ë¶„í• 
    â†“  Session-level 80/10/10
Train 80 / Val 10 / Test 10 ì„¸ì…˜
    â†“
[Step 4] Turn ìƒ˜í”Œ ìƒì„±
    â†“  Context window N=8, Rule-based summary
Train 160 / Val 20 / Test 20 turn ìƒ˜í”Œ
    â†“
[Step 5] LLM-judge ë¼ë²¨ë§
    â†“  gpt-4o-mini multi-label classification
ë¼ë²¨ë§ëœ ìƒ˜í”Œ (V1~V5 binary labels)
    â†“
[Step 6] ëª¨ë¸ í•™ìŠµ
    â†“  distilroberta-base, 3 epochs
Trained Model (multi-label classifier)
    â†“
[Step 7] í…ŒìŠ¤íŠ¸ í‰ê°€
    â†“  Precision/Recall/F1 per label
ì„±ëŠ¥ ë©”íŠ¸ë¦­ (Micro F1: 0.56, Macro F1: 0.50)
```

---

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ íŒ¨í‚¤ì§€
```bash
pip install transformers torch accelerate scikit-learn pyyaml openai
```

**ë²„ì „ ì •ë³´:**
- Python: 3.8+
- transformers: 4.30+
- torch: 2.0+
- openai: 1.0+

### ì„¤ì • íŒŒì¼: `configs/poc.yaml`

```yaml
# LLM ì„¤ì •
llm:
  api_key: "your-openai-api-key"
  model: "gpt-4o-mini"
  temperature: 0.7
  max_tokens: 1000

# í•©ì„± ë°ì´í„° ìƒì„± ì„¤ì •
synthesis:
  num_sessions: 50
  violations:
    V1: 12  # Missing context
    V2: 10  # Agency violation
    V3: 12  # Low-quality empathy
    V4: 8   # Reality distortion
    V5: 8   # Crisis safety failure
  
  # ìœ„ë°˜ë³„ ì£¼ì… ìœ„ì¹˜ (í„´ ë²”ìœ„)
  sample_turn_range:
    V1: [6, 15]   # ì¤‘ë°˜ë¶€ (ë§¥ë½ íŒŒì•… ì‹¤íŒ¨ê°€ ëª…í™•)
    V2: [6, 15]   # ì¤‘ë°˜ë¶€
    V3: [3, 12]   # ì´ˆì¤‘ë°˜ (ê³µê° í•„ìš” ì‹œì )
    V4: [6, 15]   # ì¤‘ë°˜ë¶€
    V5: [8, 15]   # í›„ë°˜ë¶€ (ìœ„ê¸° ìƒí™© ì „ê°œ í›„)

# Turn ìƒ˜í”Œë§ ì„¤ì •
sampling:
  context_window: 8           # íƒ€ê²Ÿ í„´ ì´ì „ ìµœëŒ€ 8ê°œ í„´
  samples_per_original: 2     # ì›ë³¸ ì„¸ì…˜ë‹¹ 2ê°œ ìƒ˜í”Œ
  samples_per_synthetic: 2    # í•©ì„± ì„¸ì…˜ë‹¹ 2ê°œ ìƒ˜í”Œ (ìœ„ë°˜í„´ 1 + ëœë¤ 1)
  
  summary:
    use_llm_summary: false    # trueë©´ LLM ìš”ì•½, falseë©´ rule-based
    max_turns_for_summary: 12 # ìš”ì•½ì— í¬í•¨í•  ìµœëŒ€ í„´ ìˆ˜
    max_summary_length: 100   # Rule-based ìš”ì•½ ìµœëŒ€ ê¸¸ì´

# í•™ìŠµ ì„¤ì •
training:
  model_name: "distilroberta-base"
  num_epochs: 3
  batch_size: 16
  learning_rate: 2.0e-5
  warmup_steps: 100
  weight_decay: 0.01
  max_length: 512             # í† í¬ë‚˜ì´ì € ìµœëŒ€ ê¸¸ì´
```

---

## Step 1: ì›ë³¸ ìƒ˜í”Œë§

### ëª©ì 
ESConv ì „ì²´ ë°ì´í„°ì…‹(1300 ì„¸ì…˜)ì—ì„œ ì‹¤í—˜ì— ì‚¬ìš©í•  50ê°œ ì„¸ì…˜ì„ ëœë¤ ìƒ˜í”Œë§

### ìŠ¤í¬ë¦½íŠ¸
`scripts/step1_sample_original.py`

### ì…ë ¥ ë°ì´í„°
**`ESConv.json` êµ¬ì¡°:**
```json
[
  {
    "conversation_id": "1",
    "conversation": [
      {
        "speaker": "seeker",
        "utterance_idx": 0,
        "text": "Hi, I've been feeling really down lately..."
      },
      {
        "speaker": "supporter",
        "utterance_idx": 1,
        "text": "I'm here to listen. What's been going on?"
      },
      ...
    ],
    "situation": "Financial stress due to job loss",
    "emotion_type": "anxious",
    ...
  },
  ...
]
```

- **ì´ ì„¸ì…˜ ìˆ˜**: 1300ê°œ
- **í‰ê·  í„´ ìˆ˜**: 30~40 turns/session
- **í™”ì**: seeker (ë‚´ë‹´ì), supporter (ìƒë‹´ì‚¬)

### ì²˜ë¦¬ ë¡œì§

**ì£¼ìš” ì½”ë“œ (`scripts/step1_sample_original.py`):**
```python
def sample_sessions(sessions: List[Dict], num_samples: int, seed: int) -> List[Dict]:
    """ëœë¤ ìƒ˜í”Œë§ í›„ session_id ì¬ë¶€ì—¬"""
    random.seed(seed)
    sampled = random.sample(sessions, num_samples)
    
    # Session ID ì¬ë¶€ì—¬: orig_0000, orig_0001, ...
    for i, session in enumerate(sampled):
        session['session_id'] = f"orig_{i:04d}"
    
    return sampled
```

**ì‹¤í–‰ ê³¼ì •:**
1. ESConv.json ì „ì²´ ë¡œë“œ
2. `random.sample()`ë¡œ 50ê°œ ì„ íƒ (seed=42)
3. ê° ì„¸ì…˜ì— `session_id: "orig_XXXX"` ë¶€ì—¬
4. í†µê³„ ê³„ì‚° (ì´ í„´ ìˆ˜, í‰ê·  í„´ ìˆ˜, supporter í„´ ìˆ˜)
5. JSON ì €ì¥

### ì¶œë ¥ ë°ì´í„°
**íŒŒì¼:** `data/sessions_original_50.json`

**í†µê³„:**
- ì„¸ì…˜ ìˆ˜: 50
- ì´ í„´ ìˆ˜: 1361
- í‰ê·  í„´/ì„¸ì…˜: 27.22
- Supporter í„´ ìˆ˜: 663
- Seeker í„´ ìˆ˜: 698

**ì¶œë ¥ í˜•ì‹:**
```json
[
  {
    "session_id": "orig_0000",
    "conversation": [...],
    "situation": "...",
    "emotion_type": "...",
    ...
  },
  ...
]
```

### ì‹¤í–‰ ëª…ë ¹
```bash
python scripts/step1_sample_original.py \
  --input ESConv.json \
  --output data/sessions_original_50.json \
  --num_sessions 50 \
  --seed 42
```

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
< 10ì´ˆ

### ê²€ì¦ ë°©ë²•
```bash
# ì„¸ì…˜ ìˆ˜ í™•ì¸
python -c "import json; print(len(json.load(open('data/sessions_original_50.json'))))"
# ì¶œë ¥: 50

# ì²« ë²ˆì§¸ ì„¸ì…˜ ID í™•ì¸
python -c "import json; print(json.load(open('data/sessions_original_50.json'))[0]['session_id'])"
# ì¶œë ¥: orig_0000
```

---

## Step 2: í•©ì„± ë°ì´í„° ìƒì„± (ìœ„ë°˜ ì£¼ì…)

### ëª©ì 
ì›ë³¸ ì„¸ì…˜ì˜ supporter ì‘ë‹µ 1ê°œë¥¼ LLMìœ¼ë¡œ ë¦¬ë¼ì´íŠ¸í•˜ì—¬ íŠ¹ì • ìœ„ë°˜(V1~V5) ì£¼ì…

### ìŠ¤í¬ë¦½íŠ¸ ë° ëª¨ë“ˆ
- **ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸:** `scripts/step2_generate_synthetic.py`
- **í•µì‹¬ ëª¨ë“ˆ:** `src/synth/rewrite_turn.py`
- **í”„ë¡¬í”„íŠ¸:** `src/llm/prompts.py` (REWRITE_USER_TEMPLATE)
- **LLM í´ë¼ì´ì–¸íŠ¸:** `src/llm/openai_client.py`

### ì…ë ¥ ë°ì´í„°
- `data/sessions_original_50.json` (Step 1 ì¶œë ¥)
- `configs/poc.yaml` (ìœ„ë°˜ ë¶„í¬ ì„¤ì •)

### ì²˜ë¦¬ ë¡œì§

#### 2.1 ìœ„ë°˜ íƒ€ì… í• ë‹¹
```python
# src/synth/rewrite_turn.py
def assign_violations(num_sessions: int, violation_counts: Dict) -> List[str]:
    """ì„¤ì •ëœ ë¶„í¬ëŒ€ë¡œ ìœ„ë°˜ íƒ€ì… í• ë‹¹"""
    # V1:12, V2:10, V3:12, V4:8, V5:8 â†’ ì´ 50ê°œ
    violations = []
    for v_type, count in violation_counts.items():
        violations.extend([v_type] * count)
    
    random.shuffle(violations)
    return violations  # ['V1', 'V3', 'V2', ...]
```

#### 2.2 ìœ„ë°˜ ì£¼ì… ìœ„ì¹˜ ì„ íƒ
```python
def select_violation_turn(session: Dict, violation_type: str, turn_range: Dict) -> int:
    """
    ìœ„ë°˜ íƒ€ì…ì— ì í•©í•œ supporter í„´ ì„ íƒ
    
    Args:
        session: ì„¸ì…˜ ë°ì´í„°
        violation_type: V1~V5
        turn_range: ìœ„ë°˜ë³„ í„´ ë²”ìœ„ (ì˜ˆ: V1ì€ [6, 15])
    
    Returns:
        ì„ íƒëœ supporter í„´ì˜ utterance_idx
    """
    # 1. ì „ì²´ supporter í„´ ì°¾ê¸°
    supporter_turns = [
        (i, turn) for i, turn in enumerate(session['conversation'])
        if turn['speaker'] == 'supporter'
    ]
    
    # 2. í„´ ë²”ìœ„ ë‚´ í•„í„°ë§
    min_turn, max_turn = turn_range.get(violation_type, [3, 15])
    eligible = [
        (global_idx, turn) for global_idx, turn in supporter_turns
        if min_turn <= global_idx <= max_turn
    ]
    
    # 3. ëœë¤ ì„ íƒ
    if not eligible:
        eligible = supporter_turns  # ë²”ìœ„ ë‚´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ
    
    selected_idx, _ = random.choice(eligible)
    return selected_idx
```

**ìœ„ë°˜ë³„ í„´ ë²”ìœ„ ì´ìœ :**
- **V1 (ë§¥ë½ íŒŒì•… ì‹¤íŒ¨)**: [6, 15] - ì¶©ë¶„í•œ ëŒ€í™”ê°€ ì§„í–‰ëœ í›„ ë§¥ë½ ëˆ„ë½ì´ ëª…í™•
- **V2 (ì£¼ë„ê¶Œ ì¹¨í•´)**: [6, 15] - ê´€ê³„ê°€ í˜•ì„±ëœ í›„ ì§€ì‹œì  ì¡°ì–¸ì´ ë‚˜íƒ€ë‚¨
- **V3 (ì €í’ˆì§ˆ ê³µê°)**: [3, 12] - ì´ˆë°˜~ì¤‘ë°˜, ê³µê°ì´ í•„ìš”í•œ ì‹œì 
- **V4 (í˜„ì‹¤ ì™œê³¡)**: [6, 15] - ë‚´ë‹´ìì˜ ìƒí™©ì´ ì¶©ë¶„íˆ ë“œëŸ¬ë‚œ í›„
- **V5 (ìœ„ê¸° ì•ˆì „)**: [8, 15] - ìœ„ê¸° ìƒí™©ì´ ì „ê°œëœ í›„ë°˜ë¶€

#### 2.3 LLM ë¦¬ë¼ì´íŠ¸

**í”„ë¡¬í”„íŠ¸ (`src/llm/prompts.py`):**
```python
REWRITE_USER_TEMPLATE = """You are rewriting a counseling response to inject a specific violation.

**Context:**
Situation: {situation}
Emotion: {emotion_type}

**Conversation History:**
{conversation_history}

**Original Supporter Response:**
{original_response}

**Task:**
Rewrite this response to clearly demonstrate: **{violation_type}**

{violation_description}

**Requirements:**
1. Maintain conversational flow and tone
2. Keep similar length (Â±20%)
3. Make violation obvious but realistic
4. Don't mention the violation explicitly

Return JSON:
{{
  "rewritten_response": "...",
  "rationale": "Brief explanation of how violation was injected"
}}
"""
```

**ìœ„ë°˜ë³„ ì„¤ëª… (`violation_description`):**
```python
VIOLATION_DESCRIPTIONS = {
    "V1": """Missing Context & Info-Gathering:
- Respond without understanding seeker's full situation
- Skip necessary clarifying questions
- Make assumptions about unstated details
- Jump to advice without gathering information""",
    
    "V2": """Agency Violation:
- Give directive, prescriptive advice
- Use "you should", "you must", "you need to"
- Take decision-making control from seeker
- Impose solutions without collaboration""",
    
    "V3": """Low-Quality Empathy:
- Use clichÃ©d phrases ("I understand", "Stay strong", "It'll be okay")
- Generic platitudes without personalization
- Surface-level acknowledgment
- Copy-paste emotional responses""",
    
    "V4": """Reality Distortion:
- Dismiss or minimize seeker's valid concerns
- Overly optimistic reframing
- Deny seeker's emotional reality
- "Just think positive" mentality""",
    
    "V5": """Crisis Safety Failure:
- Ignore suicide/self-harm mentions
- Respond casually to crisis signals
- Skip safety assessment
- No referral to professional help"""
}
```

**LLM í˜¸ì¶œ (`src/llm/openai_client.py`):**
```python
def call(self, system_prompt: str, user_prompt: str) -> Dict:
    """OpenAI API í˜¸ì¶œ with retry"""
    response = openai.chat.completions.create(
        model=self.model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=self.temperature,
        max_tokens=self.max_tokens
    )
    
    content = response.choices[0].message.content
    
    # JSON íŒŒì‹±
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        # Retry with JSON ê°•ì œ ë©”ì‹œì§€
        return self.call(system_prompt, RETRY_MESSAGE)
```

#### 2.4 ì„¸ì…˜ ì¬êµ¬ì„±
```python
def rewrite_session_with_violation(session: Dict, violation_type: str, config: Dict) -> Dict:
    """ì„¸ì…˜ ë³µì‚¬ í›„ 1ê°œ í„´ë§Œ ë¦¬ë¼ì´íŠ¸"""
    
    # 1. ì„¸ì…˜ ë”¥ì¹´í”¼
    new_session = copy.deepcopy(session)
    
    # 2. ìœ„ë°˜ ì£¼ì… ìœ„ì¹˜ ì„ íƒ
    target_idx = select_violation_turn(new_session, violation_type, config['turn_range'])
    
    # 3. ì›ë³¸ ì‘ë‹µ ì €ì¥
    original_text = new_session['conversation'][target_idx]['text']
    
    # 4. LLM ë¦¬ë¼ì´íŠ¸
    rewrite_result = llm_client.call(
        system_prompt=REWRITE_SYSTEM_PROMPT,
        user_prompt=format_rewrite_prompt(
            session=new_session,
            target_idx=target_idx,
            violation_type=violation_type
        )
    )
    
    # 5. í…ìŠ¤íŠ¸ êµì²´
    new_session['conversation'][target_idx]['text'] = rewrite_result['rewritten_response']
    
    # 6. ë©”íƒ€ë°ì´í„° ì¶”ê°€
    new_session['injected_violation'] = {
        'type': violation_type,
        'turn_id': target_idx,
        'supporter_utterance_index': get_supporter_index(new_session, target_idx),
        'original_text': original_text,
        'rewritten_text': rewrite_result['rewritten_response'],
        'rationale': rewrite_result['rationale']
    }
    
    # 7. Session ID ë³€ê²½
    new_session['session_id'] = new_session['session_id'].replace('orig_', 'synth_')
    
    return new_session
```

### ì¶œë ¥ ë°ì´í„°
**íŒŒì¼:** `data/sessions_synth_50.json`

**í†µê³„:**
- ì„¸ì…˜ ìˆ˜: 50
- ì„±ê³µë¥ : 100% (50/50)
- ìœ„ë°˜ ë¶„í¬:
  - V1: 12
  - V2: 10
  - V3: 12
  - V4: 8
  - V5: 8

**ì¶œë ¥ í˜•ì‹:**
```json
[
  {
    "session_id": "synth_0000",
    "conversation": [
      {"speaker": "seeker", "utterance_idx": 0, "text": "..."},
      {"speaker": "supporter", "utterance_idx": 1, "text": "... (ë¦¬ë¼ì´íŠ¸ëœ ìœ„ë°˜ ì‘ë‹µ) ..."},
      ...
    ],
    "injected_violation": {
      "type": "V1",
      "turn_id": 7,
      "supporter_utterance_index": 3,
      "original_text": "Can you tell me more about what happened?",
      "rewritten_text": "You should just move on and find a new job.",
      "rationale": "Injected V1 by skipping information gathering and jumping to advice"
    },
    "situation": "...",
    "emotion_type": "..."
  },
  ...
]
```

### ì‹¤í–‰ ëª…ë ¹
```bash
python scripts/step2_generate_synthetic.py \
  --input data/sessions_original_50.json \
  --output data/sessions_synth_50.json \
  --seed 42
```

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
- ì•½ 5~10ë¶„ (LLM API í˜¸ì¶œ 50íšŒ)
- Progress barë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ

### ì‹¤í–‰ ë¡œê·¸ ì˜ˆì‹œ
```
============================================================
STEP 2: Generate Synthetic Sessions with Violations
============================================================

Loading original sessions from: data/sessions_original_50.json
Loaded 50 sessions

Violation distribution:
  V1: 12
  V2: 10
  V3: 12
  V4: 8
  V5: 8

Generating synthetic sessions...
  [10/50] V3 injected at turn 5 (supporter_idx=2)
  [20/50] V1 injected at turn 9 (supporter_idx=4)
  [30/50] V2 injected at turn 11 (supporter_idx=5)
  ...

============================================================
Summary
============================================================
Total sessions: 50
Successful: 50
Failed: 0

Violation counts:
  V1: 12
  V2: 10
  V3: 12
  V4: 8
  V5: 8

Output saved to: data/sessions_synth_50.json
```

### ê²€ì¦ ë°©ë²•
```python
# check_synthetic.py
import json

synth = json.load(open('data/sessions_synth_50.json'))

# 1. ì „ì²´ ìˆ˜ í™•ì¸
print(f"Total sessions: {len(synth)}")

# 2. ìœ„ë°˜ ë¶„í¬ í™•ì¸
from collections import Counter
violations = [s['injected_violation']['type'] for s in synth]
print(Counter(violations))
# ì¶œë ¥: Counter({'V1': 12, 'V3': 12, 'V2': 10, 'V4': 8, 'V5': 8})

# 3. ìƒ˜í”Œ í™•ì¸
sample = synth[0]
vio = sample['injected_violation']
print(f"\nSession: {sample['session_id']}")
print(f"Violation: {vio['type']} at turn {vio['turn_id']}")
print(f"Original: {vio['original_text'][:100]}...")
print(f"Rewritten: {vio['rewritten_text'][:100]}...")
print(f"Rationale: {vio['rationale']}")
```

### ì£¼ìš” ë¬¸ì œì  ë° ëŒ€ì‘

**ë¬¸ì œ 1: LLMì´ JSON ë°˜í™˜ ì‹¤íŒ¨**
- ëŒ€ì‘: `openai_client.py`ì— retry ë¡œì§ ì¶”ê°€
- JSON parse ì‹¤íŒ¨ ì‹œ RETRY_MESSAGEë¡œ ì¬ìš”ì²­
- 2íšŒ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ â†’ í•´ë‹¹ ì„¸ì…˜ skip

**ë¬¸ì œ 2: ìœ„ë°˜ì´ ë„ˆë¬´ ì•½í•˜ê²Œ ì£¼ì…**
- ëŒ€ì‘: í”„ë¡¬í”„íŠ¸ì— "Make violation **obvious** but realistic" ê°•ì¡°
- Few-shot ì˜ˆì‹œ ì¶”ê°€ ê³ ë ¤

**ë¬¸ì œ 3: V4/V5ê°€ Step 5ì—ì„œ ì¸ì‹ ì•ˆë¨**
- í˜„ìƒ: í•©ì„±ì€ ì„±ê³µí–ˆìœ¼ë‚˜ LLM-judgeê°€ 0ê°œ íƒì§€
- ì›ì¸ ì¶”ì •:
  1. ë¦¬ë¼ì´íŠ¸ê°€ ì¶©ë¶„íˆ ê°•í•˜ì§€ ì•ŠìŒ
  2. Judge í”„ë¡¬í”„íŠ¸ê°€ V4/V5ì— ëŒ€í•´ ë„ˆë¬´ ì—„ê²©
  3. ESConv ë°ì´í„° ìì²´ì— V4/V5 íŒ¨í„´ì´ í¬ì†Œ
- ê°œì„  ë°©í–¥:
  - V4/V5 ì „ìš© few-shot ì˜ˆì‹œ ì¶”ê°€
  - Judge í”„ë¡¬í”„íŠ¸ ì™„í™”
  - ë‹¤ë¥¸ í•©ì„± ë°©ë²• ì‹œë„ (multi-turn rewrite ë“±)

---

## Step 3: Train/Val/Test ë¶„í• 

### ëª©ì 
ì„¸ì…˜ ë ˆë²¨ì—ì„œ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„í•  (ë§¥ë½ ëˆ„ì¶œ ë°©ì§€)

### ìŠ¤í¬ë¦½íŠ¸
`scripts/step3_split_sessions.py`

### ì…ë ¥ ë°ì´í„°
- `data/sessions_original_50.json`
- `data/sessions_synth_50.json`

### ì²˜ë¦¬ ë¡œì§

**í•µì‹¬ ì›ì¹™: Session-level split**
- ê°™ì€ ì„¸ì…˜ì˜ í„´ë“¤ì´ train/val/testì— ë¶„ì‚°ë˜ë©´ ì•ˆë¨
- ì„¸ì…˜ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ë¶„í• 

```python
def split_sessions(sessions: List[Dict], train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):
    """
    ì„¸ì…˜ì„ train/val/testë¡œ ë¶„í• 
    
    Args:
        sessions: ì„¸ì…˜ ë¦¬ìŠ¤íŠ¸
        train_ratio: í•™ìŠµ ë¹„ìœ¨ (0.8 = 80%)
        val_ratio: ê²€ì¦ ë¹„ìœ¨ (0.1 = 10%)
        test_ratio: í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ (0.1 = 10%)
        seed: ëœë¤ ì‹œë“œ
    
    Returns:
        train_sessions, val_sessions, test_sessions
    """
    random.seed(seed)
    shuffled = sessions.copy()
    random.shuffle(shuffled)
    
    n = len(shuffled)
    train_end = int(n * train_ratio)
    val_end = train_end + int(n * val_ratio)
    
    train = shuffled[:train_end]
    val = shuffled[train_end:val_end]
    test = shuffled[val_end:]
    
    return train, val, test
```

**ì›ë³¸/í•©ì„± ë¶„ë¦¬ ì²˜ë¦¬:**
```python
# 1. ì›ë³¸ 50ê°œ ë¶„í• 
orig_train, orig_val, orig_test = split_sessions(original_sessions, seed=42)
# ê²°ê³¼: 40, 5, 5

# 2. í•©ì„± 50ê°œ ë¶„í• 
synth_train, synth_val, synth_test = split_sessions(synthetic_sessions, seed=42)
# ê²°ê³¼: 40, 5, 5

# 3. í•©ì¹˜ê¸°
train_sessions = orig_train + synth_train      # 80 sessions
val_sessions = orig_val + synth_val            # 10 sessions
test_sessions = orig_test + synth_test         # 10 sessions
```

**ìœ„ë°˜ ë¶„í¬ í™•ì¸:**
```python
def check_violation_distribution(sessions: List[Dict]) -> Dict:
    """í•©ì„± ì„¸ì…˜ì˜ ìœ„ë°˜ ë¶„í¬ í™•ì¸"""
    violations = Counter()
    for session in sessions:
        if 'injected_violation' in session:
            violations[session['injected_violation']['type']] += 1
    return dict(violations)

# Train set í™•ì¸
train_violations = check_violation_distribution(train_sessions)
# ì˜ˆ: {'V1': 10, 'V2': 8, 'V3': 9, 'V4': 7, 'V5': 7}
```

### ì¶œë ¥ ë°ì´í„°

**íŒŒì¼:**
- `data/splits/train.json` (80 ì„¸ì…˜)
- `data/splits/val.json` (10 ì„¸ì…˜)
- `data/splits/test.json` (10 ì„¸ì…˜)

**ë¶„í•  í†µê³„:**

| Split | ì›ë³¸ | í•©ì„± | ì´ | ë¹„ìœ¨ |
|-------|------|------|-----|------|
| Train | 39   | 41   | 80  | 80%  |
| Val   | 4    | 6    | 10  | 10%  |
| Test  | 7    | 3    | 10  | 10%  |

**ìœ„ë°˜ ë¶„í¬ (í•©ì„± ì„¸ì…˜ë§Œ):**

| ìœ„ë°˜ | Train | Val | Test | ì´ |
|------|-------|-----|------|-----|
| V1   | 10    | 1   | 1    | 12  |
| V2   | 8     | 1   | 1    | 10  |
| V3   | 9     | 2   | 1    | 12  |
| V4   | 7     | 1   | 0    | 8   |
| V5   | 7     | 1   | 0    | 8   |

### ì‹¤í–‰ ëª…ë ¹
```bash
python scripts/step3_split_sessions.py \
  --original data/sessions_original_50.json \
  --synthetic data/sessions_synth_50.json \
  --output_dir data/splits \
  --seed 42
```

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
< 5ì´ˆ

### ì‹¤í–‰ ë¡œê·¸ ì˜ˆì‹œ
```
============================================================
STEP 3: Split Sessions into Train/Val/Test
============================================================

Loading sessions...
  Original: 50 sessions
  Synthetic: 50 sessions

Splitting with ratio 80/10/10 (seed=42)...

Train set (80 sessions):
  Original: 39 sessions
  Synthetic: 41 sessions
  Violations: V1=10, V2=8, V3=9, V4=7, V5=7

Val set (10 sessions):
  Original: 4 sessions
  Synthetic: 6 sessions
  Violations: V1=1, V2=1, V3=2, V4=1, V5=1

Test set (10 sessions):
  Original: 7 sessions
  Synthetic: 3 sessions
  Violations: V1=1, V2=1, V3=1, V4=0, V5=0

Output saved to: data/splits/
  - train.json
  - val.json
  - test.json
```

### ê²€ì¦ ë°©ë²•
```python
import json

train = json.load(open('data/splits/train.json'))
val = json.load(open('data/splits/val.json'))
test = json.load(open('data/splits/test.json'))

# 1. ê°œìˆ˜ í™•ì¸
print(f"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}")
# ì¶œë ¥: Train: 80, Val: 10, Test: 10

# 2. ì¤‘ë³µ í™•ì¸ (session_id ê¸°ì¤€)
train_ids = {s['session_id'] for s in train}
val_ids = {s['session_id'] for s in val}
test_ids = {s['session_id'] for s in test}

assert len(train_ids & val_ids) == 0, "Train-Val overlap!"
assert len(train_ids & test_ids) == 0, "Train-Test overlap!"
assert len(val_ids & test_ids) == 0, "Val-Test overlap!"
print("No overlap detected âœ“")

# 3. ì›ë³¸/í•©ì„± ë¹„ìœ¨ í™•ì¸
train_orig = sum(1 for s in train if s['session_id'].startswith('orig_'))
train_synth = sum(1 for s in train if s['session_id'].startswith('synth_'))
print(f"Train: {train_orig} original + {train_synth} synthetic")
```

### ì£¼ì˜ì‚¬í•­

**ì™œ Session-level splitì´ ì¤‘ìš”í•œê°€?**

âŒ **ì˜ëª»ëœ ë°©ë²• (Turn-level split):**
```python
# ëª¨ë“  í„´ì„ ì„ì–´ì„œ ë¶„í• í•˜ë©´...
all_turns = []
for session in sessions:
    for turn in session['conversation']:
        all_turns.append(turn)

train_turns, val_turns, test_turns = split(all_turns)  # WRONG!
```

**ë¬¸ì œì :**
- ê°™ì€ ì„¸ì…˜ì˜ í„´ë“¤ì´ train/val/testì— ë¶„ì‚°
- Validationì´ ì‚¬ì‹¤ìƒ í•™ìŠµ ë°ì´í„°ë¥¼ ë³´ê²Œ ë¨ (context leakage)
- ì„±ëŠ¥ì´ ê³¼ëŒ€í‰ê°€ë¨

âœ… **ì˜¬ë°”ë¥¸ ë°©ë²• (Session-level split):**
```python
# ì„¸ì…˜ ë‹¨ìœ„ë¡œ ë¶„í• 
train_sessions, val_sessions, test_sessions = split(sessions)

# ì´í›„ ê° ì„¸ì…˜ì—ì„œ í„´ ìƒ˜í”Œë§ (Step 4)
```

**íš¨ê³¼:**
- ëª¨ë¸ì´ í•™ìŠµ ì¤‘ ë³¸ ì  ì—†ëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ ëŒ€í™” í‰ê°€
- ì‹¤ì œ ë°°í¬ í™˜ê²½ê³¼ ë™ì¼í•œ ì¡°ê±´
- ì¼ë°˜í™” ì„±ëŠ¥ ì •í™•íˆ ì¸¡ì •

---

## Step 4: Turn ìƒ˜í”Œ ìƒì„±

### ëª©ì 
ê° ì„¸ì…˜ì—ì„œ supporter ì‘ë‹µì„ íƒ€ê²Ÿìœ¼ë¡œ í•˜ëŠ” turn-level ìƒ˜í”Œ ìƒì„± (context + response)

### ìŠ¤í¬ë¦½íŠ¸ ë° ëª¨ë“ˆ
- **ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸:** `scripts/step4_make_turn_samples.py`
- **í•µì‹¬ ëª¨ë“ˆ:** `src/data/make_turn_samples.py`
- **ìš”ì•½ ëª¨ë“ˆ:** `src/llm/summarize.py`

### ì…ë ¥ ë°ì´í„°
- `data/splits/train.json`
- `data/splits/val.json`
- `data/splits/test.json`
- `configs/poc.yaml`

### ìƒ˜í”Œë§ ì „ëµ

#### 4.1 ì„¸ì…˜ë³„ ìƒ˜í”Œ ê°œìˆ˜

**ì›ë³¸ ì„¸ì…˜:**
- ê° ì„¸ì…˜ì—ì„œ supporter í„´ **2ê°œ** ëœë¤ ìƒ˜í”Œë§
- ì´ìœ : ì›ë³¸ ì„¸ì…˜ì—ëŠ” ìœ„ë°˜ì´ ì—†ìœ¼ë¯€ë¡œ ë‹¤ì–‘í•œ ì‘ë‹µ íŒ¨í„´ í•™ìŠµ

**í•©ì„± ì„¸ì…˜:**
- **ìœ„ë°˜ ì£¼ì…ëœ í„´ 1ê°œ** (í•„ìˆ˜)
- **ëœë¤ í„´ 1ê°œ** (ì¶”ê°€)
- ì´ìœ : ìœ„ë°˜ í„´ì€ ë°˜ë“œì‹œ í¬í•¨ + ì •ìƒ ì‘ë‹µë„ í•¨ê»˜ í•™ìŠµ

```python
def sample_turns_from_session(session: Dict, config: Dict) -> List[int]:
    """ì„¸ì…˜ì—ì„œ íƒ€ê²Ÿ í„´ ì¸ë±ìŠ¤ ì„ íƒ"""
    
    # Supporter í„´ë“¤ì˜ global index ì°¾ê¸°
    supporter_indices = [
        i for i, turn in enumerate(session['conversation'])
        if turn['speaker'] == 'supporter'
    ]
    
    if 'injected_violation' in session:
        # í•©ì„± ì„¸ì…˜: ìœ„ë°˜ í„´ + ëœë¤ 1ê°œ
        violation_idx = session['injected_violation']['turn_id']
        
        # ìœ„ë°˜ í„´ ì œì™¸í•˜ê³  ëœë¤ ì„ íƒ
        other_indices = [idx for idx in supporter_indices if idx != violation_idx]
        random_idx = random.choice(other_indices) if other_indices else None
        
        targets = [violation_idx]
        if random_idx is not None:
            targets.append(random_idx)
    else:
        # ì›ë³¸ ì„¸ì…˜: ëœë¤ 2ê°œ
        targets = random.sample(supporter_indices, min(2, len(supporter_indices)))
    
    return targets
```

#### 4.2 Context Window (N=8)

**Sliding window ë°©ì‹:**
- íƒ€ê²Ÿ í„´ **ì´ì „** ìµœëŒ€ 8ê°œ í„´ í¬í•¨
- íƒ€ê²Ÿ í„´ ìì²´ëŠ” ì œì™¸ (ëª¨ë¸ì´ ì˜ˆì¸¡í•  ì‘ë‹µ)
- ëŒ€í™” ì‹œì‘ë¶€ë¶„ì´ë©´ ê°€ëŠ¥í•œ ë§Œí¼ë§Œ í¬í•¨

```python
def build_context_turns(conversation: List[Dict], target_idx: int, window_size: int = 8) -> List[Dict]:
    """
    íƒ€ê²Ÿ í„´ ì´ì „ ìµœëŒ€ window_sizeê°œ í„´ ì¶”ì¶œ
    
    Args:
        conversation: ì „ì²´ ëŒ€í™”
        target_idx: íƒ€ê²Ÿ í„´ì˜ global index
        window_size: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° (default: 8)
    
    Returns:
        ì»¨í…ìŠ¤íŠ¸ í„´ ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ìˆœ)
    """
    # íƒ€ê²Ÿ ì´ì „ í„´ë“¤
    prev_turns = conversation[:target_idx]
    
    # ìµœëŒ€ window_sizeê°œë§Œ
    if len(prev_turns) > window_size:
        context = prev_turns[-window_size:]
    else:
        context = prev_turns
    
    return context
```

**ì˜ˆì‹œ:**
```python
conversation = [
    {"speaker": "seeker", "text": "Hi..."},           # idx 0
    {"speaker": "supporter", "text": "Hello..."},     # idx 1
    {"speaker": "seeker", "text": "I'm sad..."},      # idx 2
    ...                                                # idx 3-10
    {"speaker": "supporter", "text": "TARGET"},       # idx 11 (íƒ€ê²Ÿ)
]

# target_idx=11, window_size=8
context = build_context_turns(conversation, 11, 8)
# ê²°ê³¼: idx 3~10 (ì´ 8ê°œ)
```

#### 4.3 ëŒ€í™” ìš”ì•½ (Summary)

**Rule-based ë°©ì‹ (í˜„ì¬ ì‚¬ìš©):**
```python
def rule_based_summary(conversation: List[Dict], max_turns: int = 12, max_length: int = 100) -> List[str]:
    """
    ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ìš”ì•½
    
    Args:
        conversation: ì „ì²´ ëŒ€í™”
        max_turns: ìš”ì•½ì— ì‚¬ìš©í•  ìµœëŒ€ í„´ ìˆ˜
        max_length: ê° bulletì˜ ìµœëŒ€ ê¸¸ì´
    
    Returns:
        ìš”ì•½ bullet points ë¦¬ìŠ¤íŠ¸
    """
    # 1. ìµœê·¼ max_turnsê°œ í„´ë§Œ ì‚¬ìš©
    recent_turns = conversation[-max_turns:]
    
    # 2. Seeker ë°œí™”ë§Œ ì¶”ì¶œ
    seeker_turns = [
        turn['text'] for turn in recent_turns
        if turn['speaker'] == 'seeker'
    ]
    
    # 3. ê° ë°œí™”ë¥¼ truncateí•´ì„œ bulletìœ¼ë¡œ
    bullets = []
    for text in seeker_turns:
        # ê¸¸ì´ ì œí•œ
        if len(text) > max_length:
            bullet = text[:max_length] + "..."
        else:
            bullet = text
        bullets.append(bullet)
    
    return bullets[:5]  # ìµœëŒ€ 5ê°œ
```

**LLM ë°©ì‹ (ì„ íƒì , `use_llm_summary: true`):**
```python
def llm_summary(conversation: List[Dict], llm_client) -> List[str]:
    """LLMìœ¼ë¡œ ëŒ€í™” ìš”ì•½ (3-5 bullet points)"""
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    conv_text = "\n".join([
        f"{turn['speaker']}: {turn['text']}"
        for turn in conversation
    ])
    
    prompt = f"""Summarize this emotional support conversation in 3-5 key points:

{conv_text}

Return JSON:
{{
  "summary": ["point 1", "point 2", ...]
}}"""
    
    result = llm_client.call(SUMMARY_SYSTEM_PROMPT, prompt)
    return result['summary']
```

**ì‚¬ìš© ì´ìœ :**
- Rule-based: ë¹ ë¦„, API ë¹„ìš© ì—†ìŒ, ì¶©ë¶„íˆ ìœ ìš©
- LLM: ë” ë‚˜ì€ í’ˆì§ˆ, í•˜ì§€ë§Œ ë¹„ìš©/ì‹œê°„ ì¦ê°€

#### 4.4 ìƒ˜í”Œ êµ¬ì¡°

```python
class TurnSample:
    """Turn-level ìƒ˜í”Œ ë°ì´í„° êµ¬ì¡°"""
    
    session_id: str           # "orig_0000" or "synth_0000"
    turn_id: int              # íƒ€ê²Ÿ í„´ì˜ global index
    
    # ì…ë ¥
    context_turns: List[Dict] # ì´ì „ 8ê°œ í„´ (ìµœëŒ€)
    summary: List[str]        # ëŒ€í™” ìš”ì•½ bullets
    response: str             # íƒ€ê²Ÿ supporter ì‘ë‹µ
    
    # ë©”íƒ€ë°ì´í„°
    meta: Dict = {
        'situation': str,         # ìƒí™© ì„¤ëª…
        'emotion_type': str,      # ê°ì • íƒ€ì…
        'is_violation_turn': bool,# ìœ„ë°˜ í„´ ì—¬ë¶€
        'num_context_turns': int, # ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ í„´ ìˆ˜
        'num_summary_points': int # ìš”ì•½ bullet ìˆ˜
    }
```

**JSON ì˜ˆì‹œ:**
```json
{
  "session_id": "synth_0005",
  "turn_id": 9,
  "context_turns": [
    {"speaker": "seeker", "text": "I lost my job last month..."},
    {"speaker": "supporter", "text": "I'm sorry to hear that..."},
    ...
  ],
  "summary": [
    "Seeker lost job last month",
    "Feeling anxious about finances",
    "Looking for support and guidance"
  ],
  "response": "You should just apply to more jobs and stop worrying.",
  "meta": {
    "situation": "Job loss causing financial stress",
    "emotion_type": "anxious",
    "is_violation_turn": true,
    "num_context_turns": 8,
    "num_summary_points": 3
  }
}
```

### ì²˜ë¦¬ íë¦„

```python
def process_split(sessions: List[Dict], config: Dict) -> List[Dict]:
    """í•œ split(train/val/test)ì˜ ëª¨ë“  ì„¸ì…˜ ì²˜ë¦¬"""
    
    all_samples = []
    
    for session in sessions:
        # 1. íƒ€ê²Ÿ í„´ ì„ íƒ
        target_indices = sample_turns_from_session(session, config)
        
        # 2. ì„¸ì…˜ ìš”ì•½ ìƒì„± (í•œ ë²ˆë§Œ)
        summary = create_session_summary(session, config)
        
        # 3. ê° íƒ€ê²Ÿ í„´ë§ˆë‹¤ ìƒ˜í”Œ ìƒì„±
        for target_idx in target_indices:
            # Context
            context_turns = build_context_turns(
                session['conversation'],
                target_idx,
                window_size=config['context_window']
            )
            
            # Response
            response = session['conversation'][target_idx]['text']
            
            # Meta
            meta = {
                'situation': session.get('situation', ''),
                'emotion_type': session.get('emotion_type', ''),
                'is_violation_turn': (
                    'injected_violation' in session and
                    session['injected_violation']['turn_id'] == target_idx
                ),
                'num_context_turns': len(context_turns),
                'num_summary_points': len(summary)
            }
            
            sample = {
                'session_id': session['session_id'],
                'turn_id': target_idx,
                'context_turns': context_turns,
                'summary': summary,
                'response': response,
                'meta': meta
            }
            
            all_samples.append(sample)
    
    return all_samples
```

### ì¶œë ¥ ë°ì´í„°

**íŒŒì¼:**
- `data/turn_samples/train.jsonl` (160 ìƒ˜í”Œ)
- `data/turn_samples/val.jsonl` (20 ìƒ˜í”Œ)
- `data/turn_samples/test.jsonl` (20 ìƒ˜í”Œ)

**í†µê³„ (Train):**
```
Total samples: 160
From original sessions: 78 (39 sessions Ã— 2)
From synthetic sessions: 82 (41 sessions Ã— 2)
  - Violation turns: 41
  - Random turns: 41

Context turns:
  Mean: 7.66
  Min: 2
  Max: 8

Summary bullets:
  Mean: 4.0
  Min: 1
  Max: 5
```

**Val/Test í†µê³„:**
```
Val: 20 samples (4 orig Ã— 2 + 6 synth Ã— 2)
Test: 20 samples (7 orig Ã— 2 + 3 synth Ã— 2)
```

### ì‹¤í–‰ ëª…ë ¹
```bash
python scripts/step4_make_turn_samples.py \
  --input_dir data/splits \
  --output_dir data/turn_samples \
  --config configs/poc.yaml
```

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
- Rule-based summary: < 30ì´ˆ
- LLM summary: ~5ë¶„ (100 ì„¸ì…˜ Ã— LLM í˜¸ì¶œ)

### ì‹¤í–‰ ë¡œê·¸ ì˜ˆì‹œ
```
============================================================
STEP 4: Create Turn-Level Samples
============================================================

Loading configuration from: configs/poc.yaml
Context window: 8
Summary method: rule-based (max_turns=12, max_length=100)

Processing train set...
  Loaded 80 sessions
  [10/80] Created 2 samples from orig_0003
  [20/80] Created 2 samples from synth_0012 (1 violation turn)
  ...
  Total train samples: 160

Processing val set...
  Loaded 10 sessions
  Total val samples: 20

Processing test set...
  Loaded 10 sessions
  Total test samples: 20

Statistics:
  Train: 160 samples (avg 7.66 context turns, 4.0 summary bullets)
  Val: 20 samples
  Test: 20 samples

Output saved to: data/turn_samples/
```

### ê²€ì¦ ë°©ë²•
```python
import json

# JSONL ë¡œë“œ
train_samples = []
with open('data/turn_samples/train.jsonl') as f:
    for line in f:
        train_samples.append(json.loads(line))

print(f"Train samples: {len(train_samples)}")

# ìœ„ë°˜ í„´ ê°œìˆ˜ í™•ì¸
violation_samples = [s for s in train_samples if s['meta']['is_violation_turn']]
print(f"Violation turns: {len(violation_samples)}")
# ì˜ˆìƒ: 41 (í•©ì„± ì„¸ì…˜ 41ê°œ Ã— 1)

# Context ê¸¸ì´ ë¶„í¬
context_lens = [s['meta']['num_context_turns'] for s in train_samples]
print(f"Context turns: mean={sum(context_lens)/len(context_lens):.2f}, min={min(context_lens)}, max={max(context_lens)}")

# ìƒ˜í”Œ í™•ì¸
sample = train_samples[0]
print(f"\nSample: {sample['session_id']} turn {sample['turn_id']}")
print(f"Context: {len(sample['context_turns'])} turns")
print(f"Summary: {len(sample['summary'])} bullets")
print(f"Response: {sample['response'][:100]}...")
print(f"Is violation: {sample['meta']['is_violation_turn']}")
```

### ì£¼ìš” ì´ìŠˆ

**ì´ìŠˆ 1: `use_llm` vs `use_llm_summary` ë³€ìˆ˜ëª… ë²„ê·¸**
- ì¦ìƒ: `src/data/make_turn_samples.py`ì—ì„œ NameError
- ì›ì¸: ì„¤ì • íŒŒì¼ì€ `use_llm_summary`ì¸ë° ì½”ë“œëŠ” `use_llm` ì‚¬ìš©
- í•´ê²°: ë³€ìˆ˜ëª… í†µì¼
```python
# Before (ë²„ê·¸)
if config['summary']['use_llm']:  # KeyError!
    ...

# After (ìˆ˜ì •)
if config['summary']['use_llm_summary']:
    ...
```

**ì´ìŠˆ 2: Contextê°€ ë„ˆë¬´ ì§§ì€ ìƒ˜í”Œ**
- ì¦ìƒ: ì¼ë¶€ ìƒ˜í”Œì˜ context_turnsê°€ 2~3ê°œ
- ì›ì¸: ëŒ€í™” ì´ˆë°˜ë¶€ í„´ì´ íƒ€ê²Ÿìœ¼ë¡œ ì„ íƒë¨
- ì˜í–¥: ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥
- ê°œì„ : ìµœì†Œ context ê¸¸ì´ í•„í„° ì¶”ê°€ ê³ ë ¤
```python
# ê°œì„ ì•ˆ
def sample_turns_from_session(session, config):
    supporter_indices = [...]
    
    # ìµœì†Œ context 5ê°œ ì´ìƒì¸ í„´ë§Œ í›„ë³´
    eligible = [
        idx for idx in supporter_indices
        if idx >= 5  # ìµœì†Œ 5ê°œ ì´ì „ í„´ ì¡´ì¬
    ]
    
    targets = random.sample(eligible, ...)
```

**ì´ìŠˆ 3: Summary í’ˆì§ˆ**
- Rule-based: ë‹¨ìˆœ truncateë¼ ë¬¸ë§¥ ì†ì‹¤
- LLM: í’ˆì§ˆ ì¢‹ì§€ë§Œ ë¹„ìš©/ì‹œê°„
- ì ˆì¶©ì•ˆ: ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ + í…œí”Œë¦¿

---

## Step 5: LLM-judge ë¼ë²¨ë§

### ëª©ì 
LLM(gpt-4o-mini)ì„ judgeë¡œ ì‚¬ìš©í•˜ì—¬ ê° supporter ì‘ë‹µì— V1~V5 ìœ„ë°˜ ì—¬ë¶€ ë¼ë²¨ë§

### ìŠ¤í¬ë¦½íŠ¸ ë° ëª¨ë“ˆ
- **ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸:** `scripts/step5_label_turns.py`
- **í•µì‹¬ ëª¨ë“ˆ:** `src/llm/judge.py`
- **í”„ë¡¬í”„íŠ¸:** `src/llm/prompts.py` (JUDGE_USER_TEMPLATE)

### ì…ë ¥ ë°ì´í„°
- `data/turn_samples/train.jsonl`
- `data/turn_samples/val.jsonl`
- `data/turn_samples/test.jsonl`
- `configs/poc.yaml`

### LLM-Judge ë°©ì‹

#### 5.1 Why LLM-Judge?

**ì „í†µì  ë¼ë²¨ë§:**
- ì‚¬ëŒì´ ì§ì ‘ 200ê°œ ìƒ˜í”Œ ë¼ë²¨ë§
- ë¹„ìš©: ì‹œê°„ë‹¹ $30 Ã— 10ì‹œê°„ = $300
- ì‹œê°„: 1ì£¼ì¼
- ì¼ê´€ì„±: ì‚¬ëŒë§ˆë‹¤ ê¸°ì¤€ ë‹¤ë¦„

**LLM-Judge ì¥ì :**
- ë¹„ìš©: 200 ìƒ˜í”Œ Ã— $0.001 = $0.20
- ì‹œê°„: ~10ë¶„
- ì¼ê´€ì„±: ë™ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ì¼ê´€ëœ ê¸°ì¤€
- í™•ì¥ì„±: 10,000ê°œë„ ì‰½ê²Œ ë¼ë²¨ë§

**ë‹¨ì :**
- ì •í™•ë„: ì‚¬ëŒë³´ë‹¤ ë‚®ì„ ìˆ˜ ìˆìŒ
- í¸í–¥: í”„ë¡¬í”„íŠ¸ í’ˆì§ˆì— ì˜ì¡´
- ê²€ì¦ í•„ìš”: ìƒ˜í”Œ ìˆ˜ë™ í™•ì¸ í•„ìˆ˜

#### 5.2 í”„ë¡¬í”„íŠ¸ ì„¤ê³„

**System Prompt:**
```python
JUDGE_SYSTEM_PROMPT = """You are an expert evaluator of emotional support conversations.

Your task is to identify violations in supporter responses across 5 categories:
- V1: Missing Context & Info-Gathering
- V2: Agency Violation
- V3: Low-Quality Empathy
- V4: Reality Distortion
- V5: Crisis Safety Failure

Evaluate objectively based on the response content and context provided.
Return results in strict JSON format."""
```

**User Prompt Template:**
```python
JUDGE_USER_TEMPLATE = """Evaluate this supporter response for violations.

**Situation:**
{situation}

**Conversation Summary:**
{summary}

**Recent Context:**
{context}

**Supporter Response to Evaluate:**
"{response}"

---

**Violation Definitions:**

**V1 - Missing Context & Info-Gathering:**
- Responds without understanding seeker's full situation
- Skips necessary clarifying questions
- Makes assumptions about unstated details
- Jumps to advice without gathering information
Examples: "Just do X", "You should Y" (without asking about constraints)

**V2 - Agency Violation:**
- Gives directive, prescriptive advice
- Uses "you should", "you must", "you need to"
- Takes decision-making control from seeker
- Imposes solutions without collaboration
Examples: "You have to leave that job", "You must talk to them now"

**V3 - Low-Quality Empathy:**
- Uses clichÃ©d phrases ("I understand", "Stay strong", "It'll be okay")
- Generic platitudes without personalization
- Surface-level acknowledgment
- Copy-paste emotional responses
Examples: "I know how you feel", "Everything happens for a reason"

**V4 - Reality Distortion:**
- Dismisses or minimizes seeker's valid concerns
- Overly optimistic reframing
- Denies seeker's emotional reality
- "Just think positive" mentality
Examples: "It's not that bad", "Just look on the bright side"

**V5 - Crisis Safety Failure:**
- Ignores suicide/self-harm mentions
- Responds casually to crisis signals
- Skips safety assessment
- No referral to professional help
Examples: Missing "I want to die" â†’ "That's tough, but..."

---

**Task:**
For each violation type (V1-V5), determine:
1. Is it present? (0 = no, 1 = yes)
2. If multiple violations exist, identify the **most severe** one
3. Extract a brief evidence span (quote from response)

**Return JSON:**
{{
  "V1": 0 or 1,
  "V2": 0 or 1,
  "V3": 0 or 1,
  "V4": 0 or 1,
  "V5": 0 or 1,
  "top_violation": "V1" or "V2" or ... or "none",
  "evidence_span": "Direct quote from response showing the violation"
}}

**Important:**
- Be strict but fair
- Multiple violations can be 1 simultaneously
- If no violations, all should be 0 and top_violation="none"
- Evidence span should be actual text from the response
"""
```

#### 5.3 ë¼ë²¨ë§ ë¡œì§

```python
def label_turn_sample(sample: Dict, llm_client, config: Dict) -> Dict:
    """
    í•˜ë‚˜ì˜ turn ìƒ˜í”Œì„ LLM-judgeë¡œ ë¼ë²¨ë§
    
    Args:
        sample: Turn ìƒ˜í”Œ (context, summary, response í¬í•¨)
        llm_client: OpenAI client
        config: ì„¤ì •
    
    Returns:
        ì›ë³¸ ìƒ˜í”Œ + labels ì¶”ê°€
    """
    # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    context_text = "\n".join([
        f"{turn['speaker']}: {turn['text']}"
        for turn in sample['context_turns']
    ])
    
    summary_text = "\n".join([
        f"- {bullet}" for bullet in sample['summary']
    ])
    
    user_prompt = JUDGE_USER_TEMPLATE.format(
        situation=sample['meta'].get('situation', 'N/A'),
        summary=summary_text,
        context=context_text,
        response=sample['response']
    )
    
    # 2. LLM í˜¸ì¶œ
    try:
        result = llm_client.call(
            system_prompt=JUDGE_SYSTEM_PROMPT,
            user_prompt=user_prompt
        )
        
        # 3. ë¼ë²¨ ì¶”ì¶œ
        labels = {
            'V1': result.get('V1', 0),
            'V2': result.get('V2', 0),
            'V3': result.get('V3', 0),
            'V4': result.get('V4', 0),
            'V5': result.get('V5', 0),
        }
        
        # 4. ìƒ˜í”Œì— ì¶”ê°€
        labeled_sample = sample.copy()
        labeled_sample['labels'] = labels
        labeled_sample['top_violation'] = result.get('top_violation', 'none')
        labeled_sample['evidence_span'] = result.get('evidence_span', '')
        
        return labeled_sample
        
    except Exception as e:
        print(f"Error labeling {sample['session_id']} turn {sample['turn_id']}: {e}")
        # ì‹¤íŒ¨ ì‹œ ëª¨ë‘ 0ìœ¼ë¡œ
        labeled_sample = sample.copy()
        labeled_sample['labels'] = {'V1': 0, 'V2': 0, 'V3': 0, 'V4': 0, 'V5': 0}
        labeled_sample['top_violation'] = 'error'
        labeled_sample['evidence_span'] = ''
        return labeled_sample
```

#### 5.4 ë°°ì¹˜ ì²˜ë¦¬

```python
def label_all_samples(input_path: Path, output_path: Path, llm_client, config):
    """JSONL íŒŒì¼ì˜ ëª¨ë“  ìƒ˜í”Œ ë¼ë²¨ë§"""
    
    # ì…ë ¥ ë¡œë“œ
    samples = []
    with open(input_path) as f:
        for line in f:
            samples.append(json.loads(line))
    
    print(f"Labeling {len(samples)} samples...")
    
    # ë¼ë²¨ë§
    labeled_samples = []
    failed_count = 0
    
    for i, sample in enumerate(samples):
        labeled = label_turn_sample(sample, llm_client, config)
        labeled_samples.append(labeled)
        
        if labeled.get('top_violation') == 'error':
            failed_count += 1
        
        # Progress
        if (i + 1) % 10 == 0:
            print(f"  [{i+1}/{len(samples)}] Labeled")
    
    # ì¶œë ¥ ì €ì¥
    with open(output_path, 'w') as f:
        for sample in labeled_samples:
            f.write(json.dumps(sample) + '\n')
    
    print(f"\nCompleted: {len(samples)} samples, {failed_count} failures")
    
    return labeled_samples
```

### ì¶œë ¥ ë°ì´í„°

**íŒŒì¼:**
- `data/labeled/labeled_turns_train.jsonl` (160 ìƒ˜í”Œ)
- `data/labeled/labeled_turns_val.jsonl` (20 ìƒ˜í”Œ)
- `data/labeled/labeled_turns_test.jsonl` (20 ìƒ˜í”Œ)

**ë¼ë²¨ ë¶„í¬ (Train 160 ìƒ˜í”Œ):**

| ìœ„ë°˜ | ê°œìˆ˜ | ë¹„ìœ¨ | Top Violation | Multi-label ì¤‘ë³µ |
|------|------|------|---------------|------------------|
| V1   | 92   | 57.5% | 85           | 52 (ë‹¤ë¥¸ ìœ„ë°˜ê³¼ ë™ì‹œ) |
| V2   | 36   | 22.5% | 23           | 13               |
| V3   | 71   | 44.4% | 45           | 26               |
| V4   | 0    | 0%    | 0            | 0                |
| V5   | 0    | 0%    | 0            | 0                |

**Multi-label í†µê³„:**
- Single violation: 120/160 (75%)
- Multiple violations: 40/160 (25%)
- No violation: 7/160 (4.4%)

**ì¶œë ¥ í˜•ì‹:**
```json
{
  "session_id": "synth_0005",
  "turn_id": 9,
  "context_turns": [...],
  "summary": [...],
  "response": "You should just apply to more jobs and stop worrying.",
  "meta": {...},
  "labels": {
    "V1": 1,
    "V2": 1,
    "V3": 0,
    "V4": 0,
    "V5": 0
  },
  "top_violation": "V1",
  "evidence_span": "just apply to more jobs and stop worrying"
}
```

### ì‹¤í–‰ ëª…ë ¹
```bash
python scripts/step5_label_turns.py \
  --input_dir data/turn_samples \
  --output_dir data/labeled \
  --config configs/poc.yaml
```

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
- 200 ìƒ˜í”Œ Ã— ~3ì´ˆ/ìƒ˜í”Œ = ~10ë¶„
- API ì†ë„ì— ë”°ë¼ ë³€ë™

### ì‹¤í–‰ ë¡œê·¸ ì˜ˆì‹œ
```
============================================================
STEP 5: Label Turn Samples with LLM-Judge
============================================================

Loading configuration from: configs/poc.yaml
LLM model: gpt-4o-mini

Labeling train set...
  Loading: data/turn_samples/train.jsonl
  Samples: 160
  [10/160] Labeled
  [20/160] Labeled
  ...
  [160/160] Labeled

  Completed: 160 samples, 0 failures

  Label distribution:
    V1: 92 (57.5%)
    V2: 36 (22.5%)
    V3: 71 (44.4%)
    V4: 0 (0.0%)    âš ï¸ WARNING
    V5: 0 (0.0%)    âš ï¸ WARNING
  
  Multi-label samples: 40/160 (25.0%)
  Top violations: V1=85, V2=23, V3=45, none=7

Labeling val set...
  [20/20] Labeled
  V1: 16, V2: 2, V3: 10, V4: 0, V5: 0

Labeling test set...
  [20/20] Labeled
  V1: 14, V2: 1, V3: 10, V4: 0, V5: 1

Output saved to: data/labeled/
```

### ê²€ì¦ ë°©ë²•

```python
import json
from collections import Counter

# ë¡œë“œ
train = []
with open('data/labeled/labeled_turns_train.jsonl') as f:
    for line in f:
        train.append(json.loads(line))

# ë¼ë²¨ ë¶„í¬
label_counts = Counter()
for sample in train:
    for v_type, value in sample['labels'].items():
        if value == 1:
            label_counts[v_type] += 1

print("Label distribution:")
for v_type in ['V1', 'V2', 'V3', 'V4', 'V5']:
    count = label_counts[v_type]
    pct = count / len(train) * 100
    print(f"  {v_type}: {count:3d} ({pct:5.1f}%)")

# Multi-label í™•ì¸
multi_label_count = 0
for sample in train:
    total_violations = sum(sample['labels'].values())
    if total_violations > 1:
        multi_label_count += 1

print(f"\nMulti-label samples: {multi_label_count}/{len(train)}")

# ìƒ˜í”Œ í™•ì¸
print("\n=== Sample with V1+V2 ===")
for sample in train:
    if sample['labels']['V1'] == 1 and sample['labels']['V2'] == 1:
        print(f"Response: {sample['response'][:150]}...")
        print(f"Top: {sample['top_violation']}")
        print(f"Evidence: {sample['evidence_span']}")
        break
```

### í•µì‹¬ ë¬¸ì œì 

#### ë¬¸ì œ 1: V4/V5ê°€ 0ê°œ
**ì¦ìƒ:**
- Step 2ì—ì„œ V4:8, V5:8 ì£¼ì…í–ˆëŠ”ë°
- LLM-judgeê°€ V4:0, V5:0 íƒì§€

**ì›ì¸ ë¶„ì„:**
1. **ë¦¬ë¼ì´íŠ¸ í’ˆì§ˆ ë¬¸ì œ**
   - V4/V5 ìœ„ë°˜ì´ ë„ˆë¬´ ì•½í•˜ê²Œ ì£¼ì…ë¨
   - í”„ë¡¬í”„íŠ¸ì˜ "realistic" ê°•ì¡°ë¡œ ë„ˆë¬´ ë¯¸ë¬˜í•˜ê²Œ ë¦¬ë¼ì´íŠ¸
   
2. **Judge í”„ë¡¬í”„íŠ¸ ë¬¸ì œ**
   - V4/V5 ì •ì˜ê°€ ë„ˆë¬´ ì—„ê²©
   - ì˜ˆì‹œê°€ ë¶€ì¡±í•´ì„œ íŒë‹¨ ê¸°ì¤€ ëª¨í˜¸
   
3. **ESConv ë°ì´í„° íŠ¹ì„±**
   - ì›ë³¸ ë°ì´í„°ê°€ ìœ„ê¸° ìƒí™© ê±°ì˜ ì—†ìŒ (V5)
   - í˜„ì‹¤ ì™œê³¡ë„ ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚¨ (V4)

**ê²€ì¦:**
```python
# V4/V5 ì£¼ì…ëœ í„´ ìˆ˜ë™ í™•ì¸
synth = json.load(open('data/sessions_synth_50.json'))

v4_sessions = [s for s in synth if s['injected_violation']['type'] == 'V4']
v5_sessions = [s for s in synth if s['injected_violation']['type'] == 'V5']

# V4 ì˜ˆì‹œ í™•ì¸
for s in v4_sessions[:2]:
    vio = s['injected_violation']
    print(f"\nOriginal: {vio['original_text']}")
    print(f"Rewritten: {vio['rewritten_text']}")
    print(f"Rationale: {vio['rationale']}")

# â†’ ì‹¤ì œë¡œ ìœ„ë°˜ì´ ëª…í™•í•œì§€ ì‚¬ëŒì´ íŒë‹¨
```

#### ë¬¸ì œ 2: V1 ê³¼ê²€ì¶œ
**ì¦ìƒ:**
- V1ì´ 92/160 (57.5%)ë¡œ ê³¼ë„í•˜ê²Œ ë§ìŒ
- Top violationë„ 85/160ì´ V1

**ì›ì¸ ì¶”ì •:**
- Judge í”„ë¡¬í”„íŠ¸ì˜ V1 ì •ì˜ê°€ ë„ˆë¬´ ê´‘ë²”ìœ„
- "ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘"ì˜ ê¸°ì¤€ì´ ëª¨í˜¸
- ëŒ€ë¶€ë¶„ì˜ ì‘ë‹µì—ì„œ clarifying question ë¶€ì¡±ìœ¼ë¡œ íŒë‹¨

**ê°œì„  ë°©í–¥:**
```python
# V1 ì •ì˜ ì¢íˆê¸°
"""
V1 - Missing Context (STRICT):
- Makes assumption about CRITICAL unstated details
- Gives advice that requires information NOT YET provided
- Seeker explicitly mentioned needing more info, but supporter ignored

NOT V1:
- General empathy without asking questions (this is normal)
- Building rapport before info gathering
- Reflecting on already-stated information
"""
```

#### ë¬¸ì œ 3: ì¼ê´€ì„± ë¶€ì¡±
**ì¦ìƒ:**
- ë™ì¼í•œ ì‘ë‹µ íŒ¨í„´ì— ë‹¤ë¥¸ ë¼ë²¨
- í”„ë¡¬í”„íŠ¸ ìˆœì„œ/í‘œí˜„ì— ë”°ë¼ ê²°ê³¼ ë³€ë™

**ê°œì„ :**
- Few-shot examples ì¶”ê°€
- Temperature ë‚®ì¶”ê¸° (0.7 â†’ 0.3)
- Multiple judges ì‚¬ìš© í›„ majority voting

---

## Step 6: ëª¨ë¸ í•™ìŠµ

### ëª©ì 
ë¼ë²¨ë§ëœ turn ìƒ˜í”Œë¡œ multi-label violation classifier í•™ìŠµ

### ìŠ¤í¬ë¦½íŠ¸
`scripts/step6_train.py`

### ì…ë ¥ ë°ì´í„°
- `data/labeled/labeled_turns_train.jsonl` (160 ìƒ˜í”Œ)
- `data/labeled/labeled_turns_val.jsonl` (20 ìƒ˜í”Œ)
- `configs/poc.yaml`

### ëª¨ë¸ ì•„í‚¤í…ì²˜

#### 6.1 Base Model
**distilroberta-base** (HuggingFace)
- 82M parameters (RoBERTaì˜ ê²½ëŸ‰ ë²„ì „)
- Pre-trained on ì˜ì–´ í…ìŠ¤íŠ¸
- Fast inference, good performance

#### 6.2 Classification Head
```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "distilroberta-base",
    num_labels=5,  # V1, V2, V3, V4, V5
    problem_type="multi_label_classification"
)
```

**Architecture:**
```
Input Text (tokenized)
    â†“
RoBERTa Encoder (6 layers)
    â†“
[CLS] Token Representation (768-dim)
    â†“
Linear(768 â†’ 5)
    â†“
Sigmoid Activation
    â†“
[p(V1), p(V2), p(V3), p(V4), p(V5)]  # ê°ê° 0~1
```

**Loss Function:**
Binary Cross-Entropy (BCE) - ê° ë¼ë²¨ì— ë…ë¦½ì 
```python
loss = BCEWithLogitsLoss()(logits, labels)
# labels: [0,1,0,1,0] ê°™ì€ multi-hot vector
```

### ì…ë ¥ í¬ë§· Serialization

#### 6.3 í…ìŠ¤íŠ¸ ì§ë ¬í™”

```python
def serialize_sample(sample: Dict) -> str:
    """
    ìƒ˜í”Œì„ ë‹¨ì¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ëª¨ë¸ ì…ë ¥)
    
    Format:
    [SITUATION] {situation}
    [SUMMARY] bullet points
    [CONTEXT] conversation turns
    [RESPONSE] target response
    """
    
    # 1. Situation
    situation = sample.get('meta', {}).get('situation', '')
    
    # 2. Summary bullets
    summary_bullets = sample.get('summary', [])
    summary = '\n'.join(f"- {b}" for b in summary_bullets) if summary_bullets else "(No summary)"
    
    # 3. Context turns
    context_turns = sample.get('context_turns', [])
    context_lines = []
    for turn in context_turns:
        speaker = turn.get('speaker', 'unknown')
        text = turn.get('text', '')
        context_lines.append(f"{speaker}: {text}")
    context = '\n'.join(context_lines) if context_lines else "(No context)"
    
    # 4. Target response
    response = sample.get('response', '')
    
    # Combine
    return f"""[SITUATION]
{situation}

[SUMMARY]
{summary}

[CONTEXT]
{context}

[RESPONSE]
{response}"""
```

**ì˜ˆì‹œ ì…ë ¥:**
```
[SITUATION]
Job loss causing financial stress

[SUMMARY]
- Seeker lost job last month
- Feeling anxious about finances
- Looking for support and guidance

[CONTEXT]
seeker: I lost my job last month and I'm really worried.
supporter: I'm sorry to hear that. Can you tell me more?
seeker: I don't know how I'll pay rent next month.
supporter: That sounds really stressful.

[RESPONSE]
You should just apply to more jobs and stop worrying.
```

### Dataset í´ë˜ìŠ¤

```python
class ViolationDataset(Dataset):
    """PyTorch Dataset for violation detection"""
    
    def __init__(self, samples: List[Dict], tokenizer, max_length: int = 512):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. Serialize to text
        text = serialize_sample(sample)
        
        # 2. Tokenize
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # 3. Extract labels
        labels = sample.get('labels', {})
        label_vector = [
            labels.get('V1', 0),
            labels.get('V2', 0),
            labels.get('V3', 0),
            labels.get('V4', 0),
            labels.get('V5', 0),
        ]
        
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(label_vector, dtype=torch.float)
        }
```

### í•™ìŠµ ì„¤ì •

#### 6.4 TrainingArguments

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="models/detector",
    
    # Epochs & Batch
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    
    # Optimizer
    learning_rate=2e-5,
    warmup_steps=100,
    weight_decay=0.01,
    
    # Evaluation
    eval_strategy="epoch",          # Validate every epoch
    save_strategy="epoch",          # Save checkpoint every epoch
    load_best_model_at_end=True,
    metric_for_best_model="micro_f1",
    greater_is_better=True,
    
    # Logging
    logging_steps=10,               # Log every 10 steps
    logging_dir="models/detector/logs",
    
    # Early Stopping
    save_total_limit=2,             # Keep only 2 best checkpoints
    
    # Hardware
    fp16=False,                     # Mixed precision (GPU only)
    dataloader_num_workers=0,
    
    # Misc
    report_to="none",               # Disable wandb/tensorboard
    seed=42
)
```

#### 6.5 Metrics

```python
def compute_metrics(eval_pred):
    """
    Evaluation ì‹œ í˜¸ì¶œë˜ëŠ” metric í•¨ìˆ˜
    
    Args:
        eval_pred: (predictions, labels) tuple
            predictions: (N, 5) logits
            labels: (N, 5) binary
    """
    logits, labels = eval_pred
    
    # Sigmoid + threshold
    probs = 1 / (1 + np.exp(-logits))
    preds = (probs >= 0.5).astype(int)
    
    # Micro F1 (ì „ì²´ ì˜ˆì¸¡ì˜ í‰ê· )
    micro_f1 = f1_score(labels.flatten(), preds.flatten(), average='micro')
    
    # Macro F1 (ê° ë¼ë²¨ F1ì˜ í‰ê· )
    macro_f1 = f1_score(labels, preds, average='macro', zero_division=0)
    
    # Per-label F1
    per_label_f1 = f1_score(labels, preds, average=None, zero_division=0)
    
    return {
        'micro_f1': micro_f1,
        'macro_f1': macro_f1,
        'V1_f1': per_label_f1[0],
        'V2_f1': per_label_f1[1],
        'V3_f1': per_label_f1[2],
        'V4_f1': per_label_f1[3],
        'V5_f1': per_label_f1[4],
    }
```

### Trainer ì´ˆê¸°í™”

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# Train
trainer.train()

# Save final model
trainer.save_model("models/detector/final_model")
```

### ì¶œë ¥ ë°ì´í„°

**íŒŒì¼:**
- `models/detector/checkpoint-10/` (epoch 1)
- `models/detector/checkpoint-20/` (epoch 2)
- `models/detector/checkpoint-30/` (epoch 3, **best**)
- `models/detector/final_model/` (ìµœì¢… ëª¨ë¸)
- `models/detector/train_metrics.json`
- `models/detector/logs/` (TensorBoard logs)

**í•™ìŠµ ë¡œê·¸ (Epoch 3):**
```
Epoch 3/3:
Step 10: loss=0.7264, lr=1.8e-06
Step 20: loss=0.7171, lr=3.8e-06
Step 30: loss=0.6903, lr=5.8e-06

Eval:
  eval_loss: 0.6705
  eval_micro_f1: 0.3529
  eval_macro_f1: 0.1697
  eval_V1_f1: 0.0000
  eval_V2_f1: 0.1818
  eval_V3_f1: 0.6667  â† V3ë§Œ í•™ìŠµë¨
  eval_V4_f1: 0.0000
  eval_V5_f1: 0.0000
```

### ì‹¤í–‰ ëª…ë ¹
```bash
python scripts/step6_train.py \
  --input_dir data/labeled \
  --output_dir models/detector \
  --config configs/poc.yaml
```

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
- CPU: ~15ë¶„
- GPU (T4): ~5ë¶„

### ì‹¤í–‰ ë¡œê·¸ ì˜ˆì‹œ
```
============================================================
STEP 6: Train Violation Detector
============================================================

Loading training data from: data/labeled
  Train samples: 160
  Val samples: 20

Loading tokenizer: distilroberta-base
Creating datasets (max_length=512)...

Initializing model: distilroberta-base
Model parameters: 82M

Training configuration:
  Epochs: 3
  Batch size: 16
  Learning rate: 2e-5
  Output dir: models\detector

Starting training...
(This may take several minutes depending on hardware)

{'loss': '0.7264', 'grad_norm': '1.602', 'learning_rate': '1.8e-06', 'epoch': '1'}
{'eval_loss': '0.7283', 'eval_micro_f1': '0.2222', 'eval_macro_f1': '0.1697', ...}

{'loss': '0.7171', 'grad_norm': '1.837', 'learning_rate': '3.8e-06', 'epoch': '2'}
{'eval_loss': '0.7084', 'eval_micro_f1': '0.2727', 'eval_macro_f1': '0.1697', ...}

{'loss': '0.6903', 'grad_norm': '1.739', 'learning_rate': '5.8e-06', 'epoch': '3'}
{'eval_loss': '0.6705', 'eval_micro_f1': '0.3529', 'eval_macro_f1': '0.1697', ...}

Training runtime: 5 min 27 sec
Saving final model to: models\detector\final_model

âœ… Step 6 complete!
```

### ê²€ì¦ ë°©ë²•

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 1. ëª¨ë¸ ë¡œë“œ
model = AutoModelForSequenceClassification.from_pretrained("models/detector/final_model")
tokenizer = AutoTokenizer.from_pretrained("models/detector/final_model")

# 2. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
test_text = """[SITUATION]
Feeling lonely after moving to new city

[SUMMARY]
- Moved 3 months ago
- Having trouble making friends
- Feeling isolated

[CONTEXT]
seeker: I don't know anyone here.
supporter: That must be hard.

[RESPONSE]
You should just go out more and talk to people.
"""

inputs = tokenizer(test_text, return_tensors="pt")
outputs = model(**inputs)
probs = torch.sigmoid(outputs.logits)[0].tolist()

print("Predictions:")
for i, v in enumerate(['V1', 'V2', 'V3', 'V4', 'V5']):
    print(f"  {v}: {probs[i]:.4f} {'âœ“' if probs[i] > 0.5 else ''}")
```

### í•µì‹¬ ë¬¸ì œì 

**ë¬¸ì œ 1: V1 í•™ìŠµ ì‹¤íŒ¨**
- ë¼ë²¨: 92/160 (ê°€ì¥ ë§ìŒ)
- ëª¨ë¸ ì˜ˆì¸¡: 0ê°œ
- ì›ì¸: ê·¹ì‹¬í•œ class imbalance + ë¼ë²¨ í’ˆì§ˆ ë¬¸ì œ

**ë¬¸ì œ 2: V2/V3ë§Œ í•™ìŠµ**
- V3 F1: 0.67 (ê´œì°®ìŒ)
- V2 F1: 0.18 (ë‚®ìŒ)
- ëª¨ë¸ì´ ê±°ì˜ ëª¨ë“  ìƒ˜í”Œì„ V2/V3ë¡œ ë¶„ë¥˜

**ë¬¸ì œ 3: ì‘ì€ ë°ì´í„°ì…‹**
- 160 train ìƒ˜í”Œì€ ë¶€ì¡±
- ì›ë˜ ê³„íš: 200+200 ì„¸ì…˜ â†’ ì¶•ì†Œ

### ê°œì„  ë°©í–¥

**Data:**
- 200+200 or 500+500 ì„¸ì…˜ìœ¼ë¡œ ì¦ëŸ‰
- Class balancing (undersample V3, oversample V4/V5)

**Model:**
- Class weights ì ìš©
```python
# V3ëŠ” weight ë‚®ê²Œ, V1/V4/V5ëŠ” ë†’ê²Œ
class_weights = torch.tensor([2.0, 1.5, 0.5, 3.0, 3.0])
```

**Training:**
- More epochs (3 â†’ 5)
- Different LR schedule
- Focal loss (hard examplesì— ì§‘ì¤‘)

**Evaluation:**
- Threshold tuning (0.5 â†’ labelë³„ ìµœì ê°’)
- Ensemble (ì—¬ëŸ¬ ëª¨ë¸ í‰ê· )

---

## Step 7: í…ŒìŠ¤íŠ¸ í‰ê°€

### ëª©ì 
í•™ìŠµëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ í‰ê°€í•˜ì—¬ ìµœì¢… ì„±ëŠ¥ ì¸¡ì •

### ìŠ¤í¬ë¦½íŠ¸
`scripts/step7_evaluate.py`

### ì…ë ¥ ë°ì´í„°
- `data/labeled/labeled_turns_test.jsonl` (20 ìƒ˜í”Œ)
- `models/detector/final_model/`
- `configs/poc.yaml`

### í‰ê°€ ì§€í‘œ

#### 7.1 Multi-label Metrics

**Precision (ì •ë°€ë„):**
```
P = TP / (TP + FP)
ì˜ˆì¸¡í•œ ìœ„ë°˜ ì¤‘ ì‹¤ì œë¡œ ìœ„ë°˜ì¸ ë¹„ìœ¨
```

**Recall (ì¬í˜„ìœ¨):**
```
R = TP / (TP + FN)
ì‹¤ì œ ìœ„ë°˜ ì¤‘ ëª¨ë¸ì´ ì°¾ì•„ë‚¸ ë¹„ìœ¨
```

**F1 Score:**
```
F1 = 2 * (P * R) / (P + R)
ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· 
```

**Micro Average:**
- ëª¨ë“  ë¼ë²¨ì˜ TP/FP/FNì„ í•©ì‚° í›„ ê³„ì‚°
- ìƒ˜í”Œ ìˆ˜ê°€ ë§ì€ ë¼ë²¨ì— ê°€ì¤‘ì¹˜

**Macro Average:**
- ê° ë¼ë²¨ì˜ metricì„ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚° í›„ í‰ê· 
- ëª¨ë“  ë¼ë²¨ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜

#### 7.2 Threshold

**Binary classification from probabilities:**
```python
# Model output: [0.1, 0.8, 0.6, 0.2, 0.05]
# Threshold: 0.5

predictions = [
    0,  # V1: 0.1 < 0.5
    1,  # V2: 0.8 >= 0.5
    1,  # V3: 0.6 >= 0.5
    0,  # V4: 0.2 < 0.5
    0,  # V5: 0.05 < 0.5
]
```

**í˜„ì¬:** ëª¨ë“  ë¼ë²¨ì— 0.5 ì‚¬ìš©  
**ê°œì„ :** ë¼ë²¨ë³„ ìµœì  threshold íƒìƒ‰

### í‰ê°€ ë¡œì§

```python
def evaluate_model(model_path: Path, test_path: Path, tokenizer):
    """ëª¨ë¸ í‰ê°€ ë° ë©”íŠ¸ë¦­ ê³„ì‚°"""
    
    # 1. ë°ì´í„° ë¡œë“œ
    test_samples = load_jsonl(test_path)
    
    # 2. ëª¨ë¸ ë¡œë“œ
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.eval()
    
    # 3. ì¶”ë¡ 
    all_predictions = []
    all_labels = []
    
    for sample in test_samples:
        # Serialize & tokenize
        text = serialize_sample(sample)
        inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
        
        # Forward
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits[0].cpu().numpy()
        
        # Sigmoid
        probs = 1 / (1 + np.exp(-logits))
        
        # Ground truth
        labels = [
            sample['labels']['V1'],
            sample['labels']['V2'],
            sample['labels']['V3'],
            sample['labels']['V4'],
            sample['labels']['V5'],
        ]
        
        all_predictions.append(probs)
        all_labels.append(labels)
    
    # 4. Binarize predictions
    all_predictions = np.array(all_predictions)  # (N, 5)
    all_labels = np.array(all_labels)            # (N, 5)
    pred_binary = (all_predictions >= 0.5).astype(int)
    
    # 5. Compute metrics
    from sklearn.metrics import precision_recall_fscore_support
    
    # Overall
    p_micro, r_micro, f1_micro, _ = precision_recall_fscore_support(
        all_labels.flatten(), pred_binary.flatten(),
        average='micro', zero_division=0
    )
    
    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(
        all_labels.flatten(), pred_binary.flatten(),
        average='macro', zero_division=0
    )
    
    # Per-label
    per_label_metrics = {}
    for i, v_name in enumerate(['V1', 'V2', 'V3', 'V4', 'V5']):
        p, r, f1, support = precision_recall_fscore_support(
            all_labels[:, i], pred_binary[:, i],
            average='binary', zero_division=0
        )
        
        per_label_metrics[v_name] = {
            'precision': float(p),
            'recall': float(r),
            'f1': float(f1),
            'support': int(all_labels[:, i].sum()),
            'predicted_positives': int(pred_binary[:, i].sum()),
            'true_positives': int(((all_labels[:, i] == 1) & (pred_binary[:, i] == 1)).sum())
        }
    
    return {
        'micro': {'precision': p_micro, 'recall': r_micro, 'f1': f1_micro},
        'macro': {'precision': p_macro, 'recall': r_macro, 'f1': f1_macro},
        'per_label': per_label_metrics
    }
```

### í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ êµ¬ì„±

**Ground Truth ë¶„í¬:**
```
V1: 14 samples (70%)
V2: 1 sample (5%)
V3: 10 samples (50%)
V4: 0 samples (0%)
V5: 1 sample (5%)

Total samples: 20
Multi-label: ~10 samples
```

### ìµœì¢… ê²°ê³¼

#### 7.3 í…ŒìŠ¤íŠ¸ ì„±ëŠ¥

**Overall Metrics:**
```
Metric       Micro   Macro
-------------------------------
Precision    0.5600  0.5125
Recall       0.5600  0.5156
F1 Score     0.5600  0.5025
```

**Per-Label Results:**

| Label | Precision | Recall | F1    | Support | Predicted | TP |
|-------|-----------|--------|-------|---------|-----------|-----|
| V1    | 0.0000    | 0.0000 | 0.0000| 14      | 0         | 0   |
| V2    | 0.0500    | 1.0000 | 0.0952| 1       | 20        | 1   |
| V3    | 0.5000    | 1.0000 | 0.6667| 10      | 20        | 10  |
| V4    | 0.0000    | 0.0000 | 0.0000| 0       | 0         | 0   |
| V5    | 0.0000    | 0.0000 | 0.0000| 1       | 0         | 0   |

**í•´ì„:**
- **V1 (14ê°œ ground truth):** 0ê°œ ì˜ˆì¸¡ â†’ ì™„ì „ ì‹¤íŒ¨
- **V2 (1ê°œ):** 20ê°œ ì˜ˆì¸¡ â†’ ê·¹ì‹¬í•œ ê³¼ê²€ì¶œ, ìš´ ì¢‹ê²Œ 1ê°œ ë§ì¶¤
- **V3 (10ê°œ):** 20ê°œ ì˜ˆì¸¡ â†’ ê³¼ê²€ì¶œ, í•˜ì§€ë§Œ 10/10 ëª¨ë‘ ì°¾ìŒ (100% recall)
- **V4:** ë°ì´í„° ì—†ìŒ
- **V5 (1ê°œ):** 0ê°œ ì˜ˆì¸¡ â†’ ì°¾ì§€ ëª»í•¨

### ì¶œë ¥ ë°ì´í„°

**íŒŒì¼:**
- `models/detector/test_results.json` (ë©”íŠ¸ë¦­ JSON)
- `models/detector/test_predictions.jsonl` (ìƒ˜í”Œë³„ ì˜ˆì¸¡ê°’)

**test_results.json:**
```json
{
  "metrics": {
    "micro": {"precision": 0.56, "recall": 0.56, "f1": 0.56},
    "macro": {"precision": 0.5125, "recall": 0.5156, "f1": 0.5025},
    "per_label": {
      "V1": {"precision": 0.0, "recall": 0.0, "f1": 0.0, "support": 14},
      "V2": {"precision": 0.05, "recall": 1.0, "f1": 0.0952, "support": 1},
      "V3": {"precision": 0.5, "recall": 1.0, "f1": 0.6667, "support": 10},
      "V4": {"precision": 0.0, "recall": 0.0, "f1": 0.0, "support": 0},
      "V5": {"precision": 0.0, "recall": 0.0, "f1": 0.0, "support": 1}
    }
  },
  "num_samples": 20,
  "label_distribution": {"V1": 14, "V2": 1, "V3": 10, "V4": 0, "V5": 1}
}
```

**test_predictions.jsonl (ì˜ˆì‹œ):**
```json
{
  "sample_id": "synth_0003_9",
  "predictions": {"V1": 0.12, "V2": 0.87, "V3": 0.92, "V4": 0.05, "V5": 0.01},
  "labels": {"V1": 1, "V2": 1, "V3": 0, "V4": 0, "V5": 0}
}
```

### ì‹¤í–‰ ëª…ë ¹
```bash
python scripts/step7_evaluate.py \
  --input_dir data/labeled \
  --model_dir models/detector \
  --output_dir models/detector \
  --config configs/poc.yaml
```

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„
< 1ë¶„ (20 ìƒ˜í”Œë§Œ)

### ì‹¤í–‰ ë¡œê·¸ ì˜ˆì‹œ
```
============================================================
STEP 7: Evaluate Violation Detector
============================================================

Loading test data from: data\labeled\labeled_turns_test.jsonl
  Test samples: 20

Test label distribution:
  V1: 14
  V2: 1
  V3: 10
  V4: 0
  V5: 1

Loading model from: models\detector\final_model
Model loaded on: cpu

Running inference on 20 samples...
  Processed 10/20
  Processed 20/20

Inference complete!

Computing metrics...

Results saved to: models\detector\test_results.json
Predictions saved to: models\detector\test_predictions.jsonl

============================================================
Test Results
============================================================

Overall Metrics:
  Micro - P: 0.5600, R: 0.5600, F1: 0.5600
  Macro - P: 0.5125, R: 0.5156, F1: 0.5025

Per-Label Metrics:
Label    Precision    Recall       F1           Support    Predicted  TP
--------------------------------------------------------------------------------
V1       0.0000       0.0000       0.0000       14         0          0
V2       0.0500       1.0000       0.0952       1          20         1
V3       0.5000       1.0000       0.6667       10         20         10
V4       0.0000       0.0000       0.0000       0          0          0
V5       0.0000       0.0000       0.0000       1          0          0

============================================================
âœ… Step 7 complete!
============================================================
```

### ê²€ì¦ ë°©ë²•

```python
import json

# 1. ê²°ê³¼ ë¡œë“œ
with open('models/detector/test_results.json') as f:
    results = json.load(f)

print("Micro F1:", results['metrics']['micro']['f1'])
print("Macro F1:", results['metrics']['macro']['f1'])

# 2. ì˜ˆì¸¡ê°’ í™•ì¸
predictions = []
with open('models/detector/test_predictions.jsonl') as f:
    for line in f:
        predictions.append(json.loads(line))

# V2/V3 ê³¼ê²€ì¶œ í™•ì¸
v2_preds = [p for p in predictions if p['predictions']['V2'] >= 0.5]
v3_preds = [p for p in predictions if p['predictions']['V3'] >= 0.5]

print(f"\nV2 predicted: {len(v2_preds)}/20")  # 20
print(f"V3 predicted: {len(v3_preds)}/20")  # 20

# 3. False Positives ë¶„ì„
v2_fps = [
    p for p in predictions
    if p['predictions']['V2'] >= 0.5 and p['labels']['V2'] == 0
]
print(f"V2 False Positives: {len(v2_fps)}")  # 19

# 4. Confusion matrix (V3 ì˜ˆì‹œ)
tp = sum(1 for p in predictions if p['predictions']['V3'] >= 0.5 and p['labels']['V3'] == 1)
fp = sum(1 for p in predictions if p['predictions']['V3'] >= 0.5 and p['labels']['V3'] == 0)
fn = sum(1 for p in predictions if p['predictions']['V3'] < 0.5 and p['labels']['V3'] == 1)
tn = sum(1 for p in predictions if p['predictions']['V3'] < 0.5 and p['labels']['V3'] == 0)

print(f"\nV3 Confusion Matrix:")
print(f"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")
# TP: 10, FP: 10, FN: 0, TN: 0
# â†’ ëª¨ë“  ìƒ˜í”Œì„ V3 positiveë¡œ ë¶„ë¥˜
```

### í•µì‹¬ ë°œê²¬

**ë°œê²¬ 1: ê·¹ì‹¬í•œ ê³¼ê²€ì¶œ**
- ëª¨ë¸ì´ ê±°ì˜ ëª¨ë“  ì‘ë‹µ(20/20)ì„ V2/V3ë¡œ ë¶„ë¥˜
- Training data bias: V3ê°€ 71/160 (44%)ë¡œ ë§ì•˜ìŒ

**ë°œê²¬ 2: V1 í•™ìŠµ ì™„ì „ ì‹¤íŒ¨**
- Ground truth 14ê°œì¸ë° 0ê°œ ì˜ˆì¸¡
- ê°€ëŠ¥í•œ ì›ì¸:
  1. LLM-judgeì˜ V1 ë¼ë²¨ë§ì´ ë¶€ì •í™•
  2. V1ì˜ íŠ¹ì§•ì´ V2/V3ê³¼ í˜¼ë™ë¨
  3. í•™ìŠµ ë°ì´í„°ì—ì„œ V1 íŒ¨í„´ ì•½í•¨

**ë°œê²¬ 3: Micro vs Macro ì°¨ì´**
- Micro F1 (0.56) > Macro F1 (0.50)
- V3ê°€ ë§ì•„ì„œ Microê°€ ë†’ê²Œ ë‚˜ì˜´
- MacroëŠ” V1/V4/V5ì˜ 0ì´ í‰ê· ì— ë°˜ì˜

**ë°œê²¬ 4: Threshold ë¬¸ì œ**
- ëª¨ë“  ë¼ë²¨ì— 0.5 threshold ì‚¬ìš©
- V2/V3ëŠ” thresholdë¥¼ ë†’ì—¬ì•¼ í•¨ (0.7~0.8)
- V1/V4/V5ëŠ” ë‚®ì¶°ì•¼ í•¨ (0.3~0.4)

---

## ğŸ” ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¶„ì„

### ì„±ê³µí•œ ë¶€ë¶„

âœ… **End-to-end íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
- 7ë‹¨ê³„ ìë™í™” ì™„ë£Œ
- ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ í”„ë ˆì„ì›Œí¬
- ì•½ 30ë¶„ ë‚´ ì „ì²´ ì‹¤í–‰ ê°€ëŠ¥ (LLM í˜¸ì¶œ ì œì™¸ ì‹œ)

âœ… **í•©ì„± ë°ì´í„° ìƒì„±**
- 50/50 ì„¸ì…˜ 100% ì„±ê³µë¥ 
- LLMì„ í™œìš©í•œ realistic violation ì£¼ì…
- Metadata trackingìœ¼ë¡œ ì¶”ì  ê°€ëŠ¥

âœ… **LLM-Judge ì‹œìŠ¤í…œ**
- 200 ìƒ˜í”Œì„ ~10ë¶„ì— ë¼ë²¨ë§
- Multi-label ì§€ì›
- Evidence span ì œê³µ

âœ… **V3 ê²€ì¶œ ì‘ë™**
- F1 0.67ë¡œ ì‹¤ìš© ê°€ëŠ¥ ìˆ˜ì¤€
- Proof-of-concept ë‹¬ì„±

### ì‹¤íŒ¨í•œ ë¶€ë¶„

âŒ **V1 í•™ìŠµ ì™„ì „ ì‹¤íŒ¨**
- Ground truth: 92/160 (ê°€ì¥ ë§ìŒ)
- ëª¨ë¸ ì˜ˆì¸¡: 0/20
- ì¶”ì • ì›ì¸:
  1. LLM-judgeì˜ V1 ê³¼ê²€ì¶œ (ë„ˆë¬´ ê´‘ë²”ìœ„í•œ ì •ì˜)
  2. V1ê³¼ V2/V3ì˜ feature í˜¼ë™
  3. Label noiseê°€ í•™ìŠµ ë°©í•´

âŒ **V4/V5 ë°ì´í„° ë¶€ì¬**
- í•©ì„±ì—ì„œ ì£¼ì…í–ˆìœ¼ë‚˜ judgeê°€ ì¸ì‹ ëª»í•¨
- í•™ìŠµ ë¶ˆê°€ëŠ¥ (0 samples)
- ì¶”ì • ì›ì¸:
  1. Rewriteê°€ ë„ˆë¬´ ì•½í•¨ (í”„ë¡¬í”„íŠ¸ì˜ "realistic" ê°•ì¡°)
  2. Judge í”„ë¡¬í”„íŠ¸ê°€ V4/V5ì— ëŒ€í•´ ë„ˆë¬´ ì—„ê²©
  3. ESConv ì›ë³¸ ë°ì´í„°ì— V4/V5 íŒ¨í„´ í¬ì†Œ

âŒ **V2/V3 ê³¼ê²€ì¶œ**
- ëª¨ë¸ì´ ê±°ì˜ ëª¨ë“  ì‘ë‹µ(20/20)ì„ V2/V3ë¡œ ë¶„ë¥˜
- Precision ë§¤ìš° ë‚®ìŒ (0.05, 0.50)
- Class imbalanceì˜ ì „í˜•ì  ì¦ìƒ

---

## ğŸ’¡ ê°œì„  ë°©í–¥

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (Short-term)

**1. ë°ì´í„° ì¦ëŸ‰**
```yaml
# configs/poc.yaml ìˆ˜ì •
sampling:
  num_sessions: 200  # 50 â†’ 200
  
synthesis:
  num_sessions: 200  # 50 â†’ 200
```
- Train: 640 ìƒ˜í”Œ (í˜„ì¬ 160ì˜ 4ë°°)
- í•™ìŠµ ì•ˆì •ì„± ì¦ê°€

**2. Class Weights ì ìš©**
```python
# scripts/step6_train.pyì— ì¶”ê°€
from torch.nn import BCEWithLogitsLoss

# ë¹ˆë„ì— ë°˜ë¹„ë¡€í•˜ëŠ” weight
# V3ê°€ ë§ìœ¼ë¯€ë¡œ ë‚®ì€ weight, V4/V5ëŠ” ë†’ì€ weight
class_weights = torch.tensor([
    2.0,  # V1: ì¤‘ê°„
    1.5,  # V2: ì¤‘ê°„
    0.5,  # V3: ë‚®ìŒ (ê³¼ê²€ì¶œ ë°©ì§€)
    3.0,  # V4: ë†’ìŒ
    3.0,  # V5: ë†’ìŒ
])

loss_fn = BCEWithLogitsLoss(pos_weight=class_weights)
```

**3. Judge í”„ë¡¬í”„íŠ¸ ê°œì„ **
```python
# src/llm/prompts.py ìˆ˜ì •

# V1 ì •ì˜ ì¢íˆê¸°
V1_STRICT = """
V1 - Missing Context (STRICT):
- Makes critical assumption about UNSTATED information
- Gives advice requiring facts NOT YET mentioned
- Seeker explicitly signals confusion, supporter ignores

NOT V1 (normal counseling):
- Building empathy before questions
- Reflecting on stated information
- General supportive responses
"""

# V4/V5 ì •ì˜ ì™„í™” + Few-shot
V4_WITH_EXAMPLES = """
V4 - Reality Distortion:
...

Examples:
- "It's not that bad" â†’ Minimizing
- "Everything happens for a reason" â†’ Denying agency
- "Just think positive!" â†’ Toxic positivity

Counter-examples (NOT V4):
- "That sounds really tough" â†’ Validation
- "How can I support you?" â†’ Open-ended
"""
```

**4. Threshold íŠœë‹**
```python
# scripts/step7_evaluate.pyì— ì¶”ê°€
from sklearn.metrics import precision_recall_curve

def find_optimal_thresholds(y_true, y_scores):
    """ê° ë¼ë²¨ë³„ ìµœì  threshold íƒìƒ‰"""
    optimal_thresholds = {}
    
    for i, v_name in enumerate(['V1', 'V2', 'V3', 'V4', 'V5']):
        precision, recall, thresholds = precision_recall_curve(
            y_true[:, i], y_scores[:, i]
        )
        
        # F1 ìµœëŒ€í™”í•˜ëŠ” threshold
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
        best_idx = np.argmax(f1_scores)
        optimal_thresholds[v_name] = thresholds[best_idx]
    
    return optimal_thresholds

# Validation setì—ì„œ threshold ì°¾ê¸°
thresholds = find_optimal_thresholds(val_labels, val_scores)
# ì˜ˆ: {'V1': 0.3, 'V2': 0.7, 'V3': 0.8, 'V4': 0.2, 'V5': 0.2}

# Testì— ì ìš©
pred_binary = (test_scores >= list(thresholds.values())).astype(int)
```

### ì¤‘ê¸° ê°œì„  (Medium-term)

**5. í•©ì„± ë°©ë²• ë‹¤ì–‘í™”**
```python
# src/synth/ ì— ìƒˆ ëª¨ë“ˆ ì¶”ê°€

# Method A: í˜„ì¬ (single-turn rewrite)
# Method B: Multi-turn rewrite
def multi_turn_rewrite(session, violation_type):
    """ì—¬ëŸ¬ í„´ì„ ì—°ì‡„ì ìœ¼ë¡œ ë¦¬ë¼ì´íŠ¸"""
    # V1 ì˜ˆì‹œ: 3ê°œ í„´ì— ê±¸ì³ ì ì§„ì ìœ¼ë¡œ ë§¥ë½ ë¬´ì‹œ
    pass

# Method C: Scratch generation
def generate_from_scratch(situation, violation_type):
    """ì£¼ì–´ì§„ ìƒí™©ì—ì„œ ìœ„ë°˜ì´ í¬í•¨ëœ ëŒ€í™” ìƒì„±"""
    pass

# Method D: Rule-based injection
def rule_based_injection(session, violation_type):
    """í…œí”Œë¦¿ ê¸°ë°˜ ìœ„ë°˜ ì£¼ì… (V2/V5ì— íš¨ê³¼ì )"""
    if violation_type == "V2":
        # "you should" íŒ¨í„´ ê°•ì œ ì‚½ì…
        response = f"You should {random.choice(DIRECTIVE_VERBS)} ..."
    pass
```

**6. Ensemble Models**
```python
# 3ê°œ ëª¨ë¸ í•™ìŠµ (ë‹¤ë¥¸ seed)
models = [
    train_model(seed=42),
    train_model(seed=123),
    train_model(seed=456)
]

# Soft voting
ensemble_probs = np.mean([m.predict(x) for m in models], axis=0)
```

**7. ì „ë¬¸ê°€ ê²€ì¦**
```python
# ìƒ˜í”Œ 100ê°œ ì„ ì • (LLM-judgeì™€ ë¶ˆì¼ì¹˜í•œ ê²ƒ ìœ„ì£¼)
ambiguous_samples = [
    s for s in labeled_samples
    if (s['meta']['is_violation_turn'] and s['labels'][violation] == 0) or
       (not s['meta']['is_violation_turn'] and sum(s['labels'].values()) > 0)
]

# ì „ë¬¸ê°€ ë¼ë²¨ë§
expert_labels = get_expert_annotations(ambiguous_samples)

# LLM-judgeì™€ ë¹„êµ
agreement = calculate_kappa(llm_labels, expert_labels)
# Îº < 0.6 â†’ LLM-judge í’ˆì§ˆ ë¬¸ì œ
```

### ì¥ê¸° ê°œì„  (Long-term)

**8. Active Learning**
```python
# 1. ëª¨ë¸ì´ ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œ ì„ ì •
uncertain = [s for s in unlabeled if entropy(model(s)) > threshold]

# 2. ì „ë¬¸ê°€ ë¼ë²¨ë§
expert_labeled = get_expert_annotations(uncertain)

# 3. ì¬í•™ìŠµ
retrain_model(train_data + expert_labeled)
```

**9. Curriculum Learning**
```python
# Easy â†’ Hard ìˆœìœ¼ë¡œ í•™ìŠµ
# Phase 1: ëª…í™•í•œ ìœ„ë°˜ë§Œ (V2/V3ì˜ ê°•í•œ ì‚¬ë¡€)
train_phase1(easy_samples, epochs=2)

# Phase 2: ëª¨ë“  ìƒ˜í”Œ
train_phase2(all_samples, epochs=3)
```

**10. í”„ë¡œë•ì…˜ ë°°í¬**
```python
# FastAPI ì„œë²„
from fastapi import FastAPI
app = FastAPI()

model = load_model("models/detector/final_model")

@app.post("/detect")
def detect_violations(conversation: List[Dict]):
    """ì‹¤ì‹œê°„ ìœ„ë°˜ íƒì§€ API"""
    # 1. ìµœê·¼ í„´ ì¶”ì¶œ
    recent_turns = conversation[-8:]
    
    # 2. ìš”ì•½ ìƒì„±
    summary = summarize(conversation)
    
    # 3. ì˜ˆì¸¡
    violations = model.predict({
        'context': recent_turns,
        'summary': summary,
        'response': conversation[-1]['text']
    })
    
    return {
        'violations': violations,
        'confidence': max(violations.values()),
        'evidence': extract_evidence(conversation[-1])
    }
```

---

## ğŸ“ˆ ì„±ëŠ¥ ì˜ˆì¸¡

### í˜„ì¬ (50+50 ì„¸ì…˜, 160 train)
```
Micro F1: 0.56
Macro F1: 0.50
V1/V4/V5: í•™ìŠµ ì‹¤íŒ¨
```

### ë°ì´í„°ë§Œ ì¦ëŸ‰ (200+200, 640 train)
```
ì˜ˆìƒ Micro F1: 0.65 (+0.09)
ì˜ˆìƒ Macro F1: 0.58 (+0.08)
V1: ì¼ë¶€ í•™ìŠµ ì‹œì‘ (F1 ~0.3)
V4/V5: ì—¬ì „íˆ ì–´ë ¤ì›€
```

### ë°ì´í„° + Class Weights + Threshold
```
ì˜ˆìƒ Micro F1: 0.70
ì˜ˆìƒ Macro F1: 0.65
V1: F1 ~0.5
V3 ê³¼ê²€ì¶œ ì™„í™”
```

### ì „ë¬¸ê°€ ë¼ë²¨ë§ 100ìƒ˜í”Œ ì¶”ê°€
```
ì˜ˆìƒ Micro F1: 0.75
ì˜ˆìƒ Macro F1: 0.70
ëª¨ë“  ë¼ë²¨ ì‹¤ìš© ìˆ˜ì¤€
```

---

## ğŸ“ Lessons Learned

### ë°ì´í„° í’ˆì§ˆì´ ê°€ì¥ ì¤‘ìš”
- 160 ìƒ˜í”Œë¡œëŠ” ë¶€ì¡±
- LLM-judgeëŠ” í¸ë¦¬í•˜ì§€ë§Œ ì „ë¬¸ê°€ ê²€ì¦ í•„ìˆ˜
- Label noiseê°€ í•™ìŠµì„ ë§ì¹  ìˆ˜ ìˆìŒ

### Multi-labelì€ Single-labelë³´ë‹¤ ì–´ë µë‹¤
- Class imbalance ë¬¸ì œ ì‹¬í™”
- Threshold tuningì´ critical
- Per-label metrics ë°˜ë“œì‹œ í™•ì¸

### í•©ì„± ë°ì´í„°ì˜ í•œê³„
- LLM rewriteëŠ” "realistic"ê³¼ "obvious" ì‚¬ì´ trade-off
- ì¼ë¶€ ìœ„ë°˜(V4/V5)ì€ rule-based ì£¼ì…ì´ ë‚˜ì„ ìˆ˜ ìˆìŒ
- ì‹¤ì œ ì‹¤íŒ¨ ì‚¬ë¡€ ìˆ˜ì§‘ì´ ê°€ì¥ ì¢‹ìŒ

### Small datasetì—ì„œì˜ ì „ëµ
- Pretrained model í•„ìˆ˜ (distilroberta)
- Data augmentation (paraphrase, backtranslation)
- Transfer learning (ê´€ë ¨ taskì—ì„œ fine-tune í›„ ì¬fine-tune)

### Evaluationì€ ë‹¨ì¼ ìˆ«ìê°€ ì•„ë‹ˆë‹¤
- Micro F1ë§Œ ë³´ë©´ ì•ˆë¨
- Per-label ë¶„ì„ í•„ìˆ˜
- Confusion matrix, False Positive ë¶„ì„
- Qualitative error analysis (ìƒ˜í”Œ ìˆ˜ë™ í™•ì¸)

---

## ğŸ”— ì°¸ê³  ìë£Œ

### ë°ì´í„°ì…‹
- **ESConv**: Liu et al. (2021), "Towards Emotional Support Dialog Systems"
  - https://github.com/thu-coai/Emotional-Support-Conversation

### ê´€ë ¨ ì—°êµ¬
- **Multi-label Text Classification**: Zhang et al. (2021)
- **LLM as Judge**: Zheng et al. (2023), "Judging LLM-as-a-Judge"
- **Synthetic Data for NLP**: Schick & SchÃ¼tze (2021), "Generating Datasets with Pretrained Language Models"

### ë„êµ¬
- **HuggingFace Transformers**: https://huggingface.co/docs/transformers
- **OpenAI API**: https://platform.openai.com/docs
- **scikit-learn**: https://scikit-learn.org/stable/modules/multiclass.html

---

## ğŸ ë‹¤ìŒ ë‹¨ê³„

### Immediate Actions
1. [ ] 200+200 ì„¸ì…˜ìœ¼ë¡œ ì¬ì‹¤í–‰
2. [ ] Class weights ì ìš©
3. [ ] Threshold íŠœë‹
4. [ ] V1/V4/V5 í”„ë¡¬í”„íŠ¸ ê°œì„ 

### Next Sprint
1. [ ] ì „ë¬¸ê°€ 100 ìƒ˜í”Œ ë¼ë²¨ë§
2. [ ] Ensemble model ì‹¤í—˜
3. [ ] Rule-based V4/V5 ì£¼ì…
4. [ ] Error analysis ë³´ê³ ì„œ

### Future Work
1. [ ] ì‹¤ì œ ìƒë‹´ ë°ì´í„° ìˆ˜ì§‘
2. [ ] Active learning íŒŒì´í”„ë¼ì¸
3. [ ] API ì„œë²„ êµ¬ì¶•
4. [ ] A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„

---

**í”„ë¡œì íŠ¸ ì™„ë£Œì¼:** 2026-01-29  
**íŒŒì´í”„ë¼ì¸ ë²„ì „:** v1.0  
**ë¬¸ì˜:** GitHub Issues (https://github.com/th610/ICMR_GenData/issues)
