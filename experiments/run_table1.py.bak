"""
Table 1: Detection Performance

Agent 파이프라인의 초기 탐지(initial_detection) 성능을 Gold Label 대비 평가.
Per-Class Precision, Recall, F1, Confusion Matrix.

Note: Table 1은 Agent가 GPT로 새로 생성한 응답에 대한 탐지 결과이므로,
      Table 2의 Gold 텍스트 직접 분류(100%)와는 다른 지표입니다.
      55%라는 수치는 GPT가 생성한 새 응답의 위반 유형이 원본과 다르기 때문.

Input:
    experiment_results/experiment_log.jsonl

Output:
    experiment_results/table1/
        table1_report.txt
        table1_metrics.json
"""
import json
import sys
import argparse
from pathlib import Path
from collections import defaultdict

RESULTS_DIR = Path(__file__).parent
LOG_PATH = RESULTS_DIR / "experiment_log.jsonl"
OUTPUT_DIR = RESULTS_DIR / "table1"

LABELS = ['normal', 'v1', 'v2', 'v3', 'v4', 'v5']
LABEL_DISPLAY = {'normal': 'Normal', 'v1': 'V1', 'v2': 'V2',
                 'v3': 'V3', 'v4': 'V4', 'v5': 'V5'}


def load_results():
    results = []
    with open(LOG_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                results.append(json.loads(line))
    return results


def compute_metrics(results):
    """Per-Class Precision, Recall, F1 + Macro F1"""
    cm = defaultdict(lambda: defaultdict(int))
    for r in results:
        gold = r['gold_label'].lower()
        pred = r['initial_detection']['predicted_label'].lower()
        cm[gold][pred] += 1

    per_class = {}
    for label in LABELS:
        tp = cm[label][label]
        fp = sum(cm[other][label] for other in LABELS if other != label)
        fn = sum(cm[label][other] for other in LABELS if other != label)
        support = sum(cm[label][pred] for pred in LABELS)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

        per_class[label] = {
            'precision': precision, 'recall': recall, 'f1': f1,
            'support': support, 'tp': tp, 'fp': fp, 'fn': fn
        }

    macro_p = sum(m['precision'] for m in per_class.values()) / len(LABELS)
    macro_r = sum(m['recall'] for m in per_class.values()) / len(LABELS)
    macro_f1 = sum(m['f1'] for m in per_class.values()) / len(LABELS)

    correct = sum(per_class[l]['tp'] for l in LABELS)
    total = len(results)
    accuracy = correct / total if total > 0 else 0.0

    cm_matrix = [[cm[gold][pred] for pred in LABELS] for gold in LABELS]

    return {
        'per_class': per_class,
        'macro_precision': macro_p,
        'macro_recall': macro_r,
        'macro_f1': macro_f1,
        'accuracy': accuracy,
        'total': total,
        'confusion_matrix': cm_matrix
    }


def format_report(m):
    lines = []
    pc = m['per_class']

    lines.append("")
    lines.append("=" * 80)
    lines.append("  TABLE 1: Detection Performance (Set A)")
    lines.append("=" * 80)
    lines.append("")

    header = f"{'Metric':<12}"
    for label in LABELS:
        header += f" {LABEL_DISPLAY[label]:>8}"
    header += f" {'Macro':>8}"
    lines.append(header)
    lines.append("-" * 80)

    for metric_name, key in [('Precision', 'precision'), ('Recall', 'recall'), ('F1-Score', 'f1')]:
        row = f"{metric_name:<12}"
        for label in LABELS:
            row += f" {pc[label][key]:>8.2f}"
        macro = sum(pc[l][key] for l in LABELS) / len(LABELS)
        row += f" {macro:>8.2f}"
        lines.append(row)

    row = f"{'Support':<12}"
    for label in LABELS:
        row += f" {pc[label]['support']:>8d}"
    row += f" {m['total']:>8d}"
    lines.append(row)

    lines.append("")
    lines.append(f"  Accuracy: {m['accuracy']:.2%} ({m['total']} samples)")
    lines.append(f"  Macro F1: {m['macro_f1']:.4f}")

    lines.append("")
    lines.append("  Confusion Matrix (Gold↓ / Pred→)")
    hdr = f"  {'':>8}"
    for l in LABELS:
        hdr += f" {LABEL_DISPLAY[l]:>6}"
    lines.append(hdr)
    lines.append("  " + "-" * 50)
    for i, gold in enumerate(LABELS):
        row = f"  {LABEL_DISPLAY[gold]:>8}"
        for j in range(len(LABELS)):
            row += f" {m['confusion_matrix'][i][j]:>6}"
        lines.append(row)
    lines.append("")

    return "\n".join(lines)


def format_latex(m):
    pc = m['per_class']
    lines = []
    lines.append(r"\begin{table}[h]")
    lines.append(r"\centering")
    lines.append(r"\caption{Detection Performance on Synthetic Test Set (N=%d)}" % m['total'])
    lines.append(r"\begin{tabular}{l" + "c" * 7 + "}")
    lines.append(r"\toprule")
    header = "Metric"
    for l in LABELS:
        header += f" & {LABEL_DISPLAY[l]}"
    header += r" & Macro \\"
    lines.append(header)
    lines.append(r"\midrule")

    for name, key in [('Precision', 'precision'), ('Recall', 'recall'), ('F1', 'f1')]:
        row = name
        for l in LABELS:
            row += f" & {pc[l][key]:.2f}"
        macro = sum(pc[l][key] for l in LABELS) / len(LABELS)
        row += f" & {macro:.2f} \\\\"
        lines.append(row)

    row = "Support"
    for l in LABELS:
        row += f" & {pc[l]['support']}"
    row += f" & {m['total']} \\\\"
    lines.append(row)

    lines.append(r"\bottomrule")
    lines.append(r"\end{tabular}")
    lines.append(r"\end{table}")
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="Table 1: Detection Performance")
    parser.add_argument('--format', choices=['text', 'latex', 'both'], default='both')
    args = parser.parse_args()

    if not LOG_PATH.exists():
        print(f"ERROR: {LOG_PATH} not found. Run run_full_experiment.py first.")
        sys.exit(1)

    results = load_results()
    print(f"Loaded {len(results)} results from {LOG_PATH}\n")

    metrics = compute_metrics(results)

    # Print
    report = format_report(metrics)
    print(report)

    # Save
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    with open(OUTPUT_DIR / "table1_report.txt", 'w', encoding='utf-8') as f:
        f.write(report)

    with open(OUTPUT_DIR / "table1_metrics.json", 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False, default=str)

    if args.format in ('latex', 'both'):
        latex = format_latex(metrics)
        print("\n[LaTeX]")
        print(latex)
        with open(OUTPUT_DIR / "table1_latex.tex", 'w') as f:
            f.write(latex)

    print(f"\n[SAVED] {OUTPUT_DIR}/")


if __name__ == "__main__":
    main()
