"""
Phase 6 Extended: LLM Judge for Quality Verification (ì „ì²´ ëŒ€í™” ë°©ì‹)

ìƒì„±ëœ íŒŒì¼ëŸ¿ ë°ì´í„°ì˜ í’ˆì§ˆì„ LLMìœ¼ë¡œ ê²€ì¦:
1. ì „ì²´ ëŒ€í™”ë¥¼ Judgeì—ê²Œ ì œê³µ
2. ë§ˆì§€ë§‰ Supporter ì‘ë‹µ í‰ê°€
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any
import sys
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.llm.openai_client import OpenAIClient
from src.llm.prompts import JUDGE_SYSTEM, build_judge_prompt


def build_judge_input(session: Dict) -> str:
    """ì „ì²´ ëŒ€í™”ë¥¼ Judge ì…ë ¥ìœ¼ë¡œ êµ¬ì„±
    
    Returns:
        ì „ì²´ ëŒ€í™” í…ìŠ¤íŠ¸
    """
    
    situation = session.get('situation', '')
    dialog = session.get('dialog', [])
    
    # ì „ì²´ ëŒ€í™” êµ¬ì„±
    dialog_lines = [f"[ìƒí™©]\n{situation}\n"]
    dialog_lines.append("[ì „ì²´ ëŒ€í™”]")
    
    for i, turn in enumerate(dialog):
        speaker = turn.get('speaker', 'unknown')
        # ë§ˆì§€ë§‰ í„´ì´ê³  text í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš© (V1-V3 ìƒì„± ë°ì´í„°)
        if i == len(dialog) - 1 and 'text' in turn:
            content = turn.get('text', '')
        else:
            content = turn.get('content', '')
        # ë§ˆì§€ë§‰ í„´ í‘œì‹œ
        marker = " â† í‰ê°€ ëŒ€ìƒ" if i == len(dialog) - 1 else ""
        dialog_lines.append(f"[{speaker.upper()}] {content}{marker}")
    
    full_dialog_text = "\n".join(dialog_lines)
    
    return full_dialog_text


class QualityJudge:
    """LLMì„ ì‚¬ìš©í•œ í’ˆì§ˆ í‰ê°€ (ìš”ì•½+ìœˆë„ìš° ë°©ì‹)"""
    
    def __init__(self, llm_client: OpenAIClient):
        self.llm_client = llm_client
        self.results = {
            'normal': [],
            'v1': [],
            'v2': [],
            'v3': [],
            'v4': [],
            'v5': []
        }
    
    def judge_session(self, session: Dict, expected_label: str) -> Dict:
        """ê°œë³„ ì„¸ì…˜ í‰ê°€ (ì „ì²´ ëŒ€í™”)"""
        
        session_id = session.get('session_id', 'unknown')
        
        try:
            # ì „ì²´ ëŒ€í™” ìƒì„±
            full_dialog_text = build_judge_input(session)
            
            # prompts.pyì˜ build_judge_prompt ì‚¬ìš©
            user_prompt = build_judge_prompt(full_dialog=full_dialog_text)
            
            # LLM í˜¸ì¶œ
            response = self.llm_client.call(
                system_prompt=JUDGE_SYSTEM,
                user_prompt=user_prompt
            )
            
            # ì›ë³¸ ë°ì´í„° ì¶”ì¶œ
            situation = session.get('situation', '')
            dialog = session.get('dialog', [])
            
            # ê²°ê³¼ íŒŒì‹± (ì›ë³¸ ë°ì´í„° í¬í•¨)
            result = {
                'session_id': session_id,
                'expected_label': expected_label,
                'predicted_label': response.get('label', 'Unknown'),  # 'label' í‚¤ ì‚¬ìš©
                'reason': response.get('reason', ''),
                'confidence': response.get('confidence', 'unknown'),
                'matches': response.get('label') == expected_label,
                # ì›ë³¸ ë°ì´í„° ì¶”ê°€
                'situation': situation,
                'dialog': dialog,
                'num_turns': len(dialog)
            }
            
            return result
            
        except Exception as e:
            print(f"  âŒ Error judging {session_id}: {e}")
            return {
                'session_id': session_id,
                'expected_label': expected_label,
                'predicted_label': 'Error',
                'reason': str(e),
                'confidence': 'error',
                'matches': False
            }
    
    def judge_all(self, data_dir: Path, sample_per_class: int = None):
        """ëª¨ë“  ì„¸ì…˜ í‰ê°€ (ë˜ëŠ” ìƒ˜í”Œë§)"""
        
        print("=" * 80)
        print("LLM Judge: Quality Verification")
        print("=" * 80)
        print()
        
        classes = ['normal', 'v1', 'v2', 'v3', 'v4', 'v5']
        
        for cls in classes:
            filepath = data_dir / f"{cls}.json"
            
            if not filepath.exists():
                print(f"âš ï¸  Skipping {cls}: file not found")
                continue
            
            # íŒŒì¼ ë¡œë“œ
            with open(filepath, 'r', encoding='utf-8') as f:
                sessions = json.load(f)
            
            # ìƒ˜í”Œë§ (ì§€ì •ë˜ë©´)
            if sample_per_class:
                sessions = sessions[:sample_per_class]
            
            print(f"\n{'â”€' * 80}")
            print(f"Judging {cls.upper()}: {len(sessions)} sessions")
            print(f"{'â”€' * 80}")
            
            # ê° ì„¸ì…˜ í‰ê°€
            for i, session in enumerate(sessions, 1):
                session_id = session.get('session_id', f'{cls}_{i}')
                print(f"\n  [{i}/{len(sessions)}] {session_id}...", end=" ", flush=True)
                
                expected_label = cls.upper() if cls != 'normal' else 'Normal'
                result = self.judge_session(session, expected_label)
                
                # ê²°ê³¼ ì €ì¥
                self.results[cls].append(result)
                
                # ê°„ë‹¨í•œ ì¶œë ¥
                matches = result.get('matches', False)
                predicted = result.get('predicted_label', 'Unknown')
                confidence = result.get('confidence', 'unknown')
                
                if matches:
                    print(f"âœ… MATCH ({confidence})")
                else:
                    print(f"âŒ MISMATCH: {expected_label} â†’ {predicted} ({confidence})")
                    reason = result.get('reason', 'N/A')[:60]
                    print(f"      ì´ìœ : {reason}...")
    
    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        
        print("\n" + "=" * 80)
        print("LLM Judge Summary")
        print("=" * 80)
        
        total_match = 0
        total_mismatch = 0
        total_error = 0
        
        print(f"\n{'Class':<10} {'Total':<8} {'Match':<8} {'Mismatch':<10} {'Error':<8} {'Accuracy':<10}")
        print("â”€" * 70)
        
        for cls in ['normal', 'v1', 'v2', 'v3', 'v4', 'v5']:
            results = self.results[cls]
            
            if not results:
                continue
            
            match_count = sum(1 for r in results if r.get('matches', False))
            error_count = sum(1 for r in results if r.get('predicted_label') == 'Error')
            mismatch_count = len(results) - match_count - error_count
            
            accuracy = (match_count / len(results) * 100) if len(results) > 0 else 0
            
            total_match += match_count
            total_mismatch += mismatch_count
            total_error += error_count
            
            print(f"{cls.upper():<10} {len(results):<8} {match_count:<8} "
                  f"{mismatch_count:<10} {error_count:<8} {accuracy:<10.1f}%")
        
        print("â”€" * 70)
        total = total_match + total_mismatch + total_error
        total_accuracy = (total_match / total * 100) if total > 0 else 0
        print(f"{'TOTAL':<10} {total:<8} {total_match:<8} "
              f"{total_mismatch:<10} {total_error:<8} {total_accuracy:<10.1f}%")
        
        # Accuracy ì¶œë ¥
        if total > 0:
            print(f"\nâœ… Overall Accuracy: {total_accuracy:.1f}% ({total_match}/{total})")
            
            if total_mismatch > 0:
                print(f"âŒ Mismatches: {total_mismatch}")
        
        # Mismatch ìƒì„¸ ì¶œë ¥
        if total_mismatch > 0:
            print("\n" + "=" * 80)
            print("Mismatch Details")
            print("=" * 80)
            
            for cls in ['normal', 'v1', 'v2', 'v3', 'v4', 'v5']:
                mismatches = [
                    r for r in self.results[cls]
                    if not r.get('matches', False) and r.get('predicted_label') != 'Error'
                ]
                
                if mismatches:
                    print(f"\n{cls.upper()}:")
                    for r in mismatches:
                        session_id = r.get('session_id', 'unknown')
                        expected = r.get('expected_label', '')
                        predicted = r.get('predicted_label', '')
                        reason = r.get('reason', 'N/A')
                        print(f"  [{session_id}] {expected} â†’ {predicted}")
                        print(f"    ì´ìœ : {reason}")
    
    def save_results(self, output_path: Path):
        """ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        
        output_data = {
            'timestamp': datetime.now().isoformat(),
            'results': self.results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to: {output_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("=" * 80)
    print("Phase 6: LLM Judge Quality Verification")
    print("ë°©ì‹: ìš”ì•½ + ìœˆë„ìš° (ì‹¤ì œ í™˜ê²½ê³¼ ë™ì¼)")
    print("=" * 80)
    print()
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬
    data_dir = Path(__file__).parent.parent.parent / "data" / "pilot"
    output_path = Path(__file__).parent.parent.parent / "data" / "pilot" / "judge_results_summary_window.json"
    
    # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    print("ğŸ”§ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”...")
    llm_client = OpenAIClient(
        model="gpt-4o-mini",
        temperature=0.3,  # í‰ê°€ëŠ” ì¼ê´€ì„± ìˆê²Œ
        max_tokens=300    # ê°„ë‹¨í•œ JSON ì‘ë‹µ
    )
    print(f"   ëª¨ë¸: gpt-4o-mini, temperature: 0.3")
    print()
    
    # Judge ì‹¤í–‰
    judge = QualityJudge(llm_client)
    
    # ì˜µì…˜: ê° í´ë˜ìŠ¤ë³„ ëª‡ ê°œë§Œ ìƒ˜í”Œë§ (ë¹„ìš© ì ˆê°)
    # Noneì´ë©´ ì „ì²´ í‰ê°€
    sample_per_class = None  # ì „ì²´ í‰ê°€ (ê° í´ë˜ìŠ¤ë³„ 5ê°œ, ì´ 30ê°œ)
    
    if sample_per_class:
        print(f"ğŸ“Š ìƒ˜í”Œë§: ê° í´ë˜ìŠ¤ë³„ {sample_per_class}ê°œ (ì´ {sample_per_class * 6}ê°œ)")
    else:
        print(f"ğŸ“Š ì „ì²´ í‰ê°€: 30ê°œ ì„¸ì…˜ (ê° í´ë˜ìŠ¤ë³„ 5ê°œ)")
    print("ğŸ’° ì˜ˆìƒ ë¹„ìš©: ~$0.40 (ìš”ì•½ ìƒì„± + Judge)")
    print()     
    
    judge.judge_all(data_dir, sample_per_class=sample_per_class)
    
    # ê²°ê³¼ ìš”ì•½
    judge.print_summary()
    
    # ê²°ê³¼ ì €ì¥
    judge.save_results(output_path)
    
    print("\n" + "=" * 80)
    print("âœ… LLM Judge completed!")
    print("=" * 80)


if __name__ == "__main__":
    main()
